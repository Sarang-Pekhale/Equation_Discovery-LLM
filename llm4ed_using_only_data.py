# -*- coding: utf-8 -*-
"""LLM4ED_using_only_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s1B8BZRZpZ4DWqKsmPMj1bEWv-nNziem

## All Imports
"""

import json
!pip install together
# import scipy.io as sio
import warnings

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/menggedu/EDL.git
# %cd EDL

# --- Core Imports ---
import numpy as np
import scipy.io as sio
import os
import sys
import re
from typing import Optional, List, Dict, Tuple, Any
import traceback
import warnings
import random
import sympy as sp
from sklearn.metrics import r2_score

"""## Helper Functions

"""

import numpy as np
from numpy import linalg as LA
import scipy.sparse as sparse
from scipy.sparse import csc_matrix
from scipy.sparse import dia_matrix
import itertools
import operator

"""
A.Roy & S.Pekhale.  2025
"""


##################################################################################
##################################################################################
#
# Functions for taking derivatives.
# When in doubt / nice data ===> finite differences
#               \ noisy data ===> polynomials
#
##################################################################################
##################################################################################

def TikhonovDiff(f, dx, lam, d = 1):
    """
    Tikhonov differentiation.

    return argmin_g \|Ag-f\|_2^2 + lam*\|Dg\|_2^2
    where A is trapezoidal integration and D is finite differences for first dervative

    It looks like it will work well and does for the ODE case but
    tends to introduce too much bias to work well for PDEs.  If the data is noisy, try using
    polynomials instead.
    """

    # Initialize a few things
    n = len(f)
    f = np.matrix(f - f[0]).reshape((n,1))

    # Get a trapezoidal approximation to an integral
    A = np.zeros((n,n))
    for i in range(1, n):
        A[i,i] = dx/2
        A[i,0] = dx/2
        for j in range(1,i): A[i,j] = dx

    e = np.ones(n-1)
    D = sparse.diags([e, -e], [1, 0], shape=(n-1, n)).todense() / dx

    # Invert to find derivative
    g = np.squeeze(np.asarray(np.linalg.lstsq(A.T.dot(A) + lam*D.T.dot(D),A.T.dot(f),rcond=None)[0]))

    if d == 1: return g

    # If looking for a higher order derivative, this one should be smooth so now we can use finite differences
    else: return FiniteDiff(g, dx, d-1)

def FiniteDiff(u, dx, d):
    """
    Takes dth derivative data using 2nd order finite difference method (up to d=3)
    Works but with poor accuracy for d > 3

    Input:
    u = data to be differentiated
    dx = Grid spacing.  Assumes uniform spacing
    """

    n = u.size
    ux = np.zeros(n, dtype=np.complex64)

    if d == 1:
        for i in range(1,n-1):
            ux[i] = (u[i+1]-u[i-1]) / (2*dx)

        ux[0] = (-3.0/2*u[0] + 2*u[1] - u[2]/2) / dx
        ux[n-1] = (3.0/2*u[n-1] - 2*u[n-2] + u[n-3]/2) / dx
        return ux

    if d == 2:
        for i in range(1,n-1):
            ux[i] = (u[i+1]-2*u[i]+u[i-1]) / dx**2

        ux[0] = (2*u[0] - 5*u[1] + 4*u[2] - u[3]) / dx**2
        ux[n-1] = (2*u[n-1] - 5*u[n-2] + 4*u[n-3] - u[n-4]) / dx**2
        return ux

    if d == 3:
        for i in range(2,n-2):
            ux[i] = (u[i+2]/2-u[i+1]+u[i-1]-u[i-2]/2) / dx**3

        ux[0] = (-2.5*u[0]+9*u[1]-12*u[2]+7*u[3]-1.5*u[4]) / dx**3
        ux[1] = (-2.5*u[1]+9*u[2]-12*u[3]+7*u[4]-1.5*u[5]) / dx**3
        ux[n-1] = (2.5*u[n-1]-9*u[n-2]+12*u[n-3]-7*u[n-4]+1.5*u[n-5]) / dx**3
        ux[n-2] = (2.5*u[n-2]-9*u[n-3]+12*u[n-4]-7*u[n-5]+1.5*u[n-6]) / dx**3
        return ux

    if d > 3:
        return FiniteDiff(FiniteDiff(u,dx,3), dx, d-3)

def ConvSmoother(x, p, sigma):
    """
    Smoother for noisy data

    Inpute = x, p, sigma
    x = one dimensional series to be smoothed
    p = width of smoother
    sigma = standard deviation of gaussian smoothing kernel
    """

    n = len(x)
    y = np.zeros(n, dtype=np.complex64)
    g = np.exp(-np.power(np.linspace(-p,p,2*p),2)/(2.0*sigma**2))

    for i in range(n):
        a = max([i-p,0])
        b = min([i+p,n])
        c = max([0, p-i])
        d = min([2*p,p+n-i])
        y[i] = np.sum(np.multiply(x[a:b], g[c:d]))/np.sum(g[c:d])

    return y

def PolyDiff(u, x, deg = 3, diff = 1, width = 5):

    """
    u = values of some function
    x = x-coordinates where values are known
    deg = degree of polynomial to use
    diff = maximum order derivative we want
    width = width of window to fit to polynomial

    This throws out the data close to the edges since the polynomial derivative only works
    well when we're looking at the middle of the points fit.
    """

    u = u.flatten()
    x = x.flatten()

    n = len(x)
    du = np.zeros((n - 2*width,diff))

    # Take the derivatives in the center of the domain
    for j in range(width, n-width):

        # Note code originally used an even number of points here.
        # This is an oversight in the original code fixed in 2022.
        points = np.arange(j - width, j + width + 1)

        # Fit to a polynomial
        poly = np.polynomial.chebyshev.Chebyshev.fit(x[points],u[points],deg)

        # Take derivatives
        for d in range(1,diff+1):
            du[j-width, d-1] = poly.deriv(m=d)(x[j])

    return du

def PolyDiffPoint(u, x, deg = 3, diff = 1, index = None):

    """
    Same as above but now just looking at a single point

    u = values of some function
    x = x-coordinates where values are known
    deg = degree of polynomial to use
    diff = maximum order derivative we want
    """

    n = len(x)
    if index == None: index = (n-1)//2

    # Fit to a polynomial
    poly = np.polynomial.chebyshev.Chebyshev.fit(x,u,deg)

    # Take derivatives
    derivatives = []
    for d in range(1,diff+1):
        derivatives.append(poly.deriv(m=d)(x[index]))

    return derivatives

##################################################################################
##################################################################################
#
# Functions specific to PDE-FIND
#
##################################################################################
##################################################################################

def build_Theta(data, derivatives, derivatives_description, P, data_description = None):
    """
    builds a matrix with columns representing polynoimials up to degree P of all variables

    This is used when we subsample and take all the derivatives point by point or if there is an
    extra input (Q in the paper) to put in.

    input:
        data: column 0 is U, and columns 1:end are Q
        derivatives: a bunch of derivatives of U and maybe Q, should start with a column of ones
        derivatives_description: description of what derivatives have been passed in
        P: max power of polynomial function of U to be included in Theta

    returns:
        Theta = Theta(U,Q)
        descr = description of what all the columns in Theta are
    """

    n,d = data.shape
    m, d2 = derivatives.shape
    if n != m: raise Exception('dimension error')
    if data_description is not None:
        if len(data_description) != d: raise Exception('data descrption error')

    # Create a list of all polynomials in d variables up to degree P
    rhs_functions = {}
    f = lambda x, y : np.prod(np.power(list(x), list(y)))
    powers = []
    for p in range(1,P+1):
            size = d + p - 1
            for indices in itertools.combinations(range(size), d-1):
                starts = [0] + [index+1 for index in indices]
                stops = indices + (size,)
                powers.append(tuple(map(operator.sub, stops, starts)))
    for power in powers: rhs_functions[power] = [lambda x, y = power: f(x,y), power]

    # First column of Theta is just ones.
    Theta = np.ones((n,1), dtype=np.complex64)
    descr = ['']

    # Add the derivaitves onto Theta
    for D in range(1,derivatives.shape[1]):
        Theta = np.hstack([Theta, derivatives[:,D].reshape(n,1)])
        descr.append(derivatives_description[D])

    # Add on derivatives times polynomials
    for D in range(derivatives.shape[1]):
        for k in rhs_functions.keys():
            func = rhs_functions[k][0]
            new_column = np.zeros((n,1), dtype=np.complex64)
            for i in range(n):
                new_column[i] = func(data[i,:])*derivatives[i,D]
            Theta = np.hstack([Theta, new_column])
            if data_description is None: descr.append(str(rhs_functions[k][1]) + derivatives_description[D])
            else:
                function_description = ''
                for j in range(d):
                    if rhs_functions[k][1][j] != 0:
                        if rhs_functions[k][1][j] == 1:
                            function_description = function_description + data_description[j]
                        else:
                            function_description = function_description + data_description[j] + '^' + str(rhs_functions[k][1][j])
                descr.append(function_description + derivatives_description[D])

    return Theta, descr

def build_linear_system(u, dt, dx, D = 3, P = 3,time_diff = 'poly',space_diff = 'poly',lam_t = None,lam_x = None, width_x = None,width_t = None, deg_x = 5,deg_t = None,sigma = 2):
    """
    Constructs a large linear system to use in later regression for finding PDE.
    This function works when we are not subsampling the data or adding in any forcing.

    Input:
        Required:
            u = data to be fit to a pde
            dt = temporal grid spacing
            dx = spatial grid spacing
        Optional:
            D = max derivative to include in rhs (default = 3)
            P = max power of u to include in rhs (default = 3)
            time_diff = method for taking time derivative
                        options = 'poly', 'FD', 'FDconv','TV'
                        'poly' (default) = interpolation with polynomial
                        'FD' = standard finite differences
                        'FDconv' = finite differences with convolutional smoothing
                                   before and after along x-axis at each timestep
                        'Tik' = Tikhonov (takes very long time)
            space_diff = same as time_diff with added option, 'Fourier' = differentiation via FFT
            lam_t = penalization for L2 norm of second time derivative
                    only applies if time_diff = 'TV'
                    default = 1.0/(number of timesteps)
            lam_x = penalization for L2 norm of (n+1)st spatial derivative
                    default = 1.0/(number of gridpoints)
            width_x = number of points to use in polynomial interpolation for x derivatives
                      or width of convolutional smoother in x direction if using FDconv
            width_t = number of points to use in polynomial interpolation for t derivatives
            deg_x = degree of polynomial to differentiate x
            deg_t = degree of polynomial to differentiate t
            sigma = standard deviation of gaussian smoother
                    only applies if time_diff = 'FDconv'
                    default = 2
    Output:
        ut = column vector of length u.size
        R = matrix with ((D+1)*(P+1)) of column, each as large as ut
        rhs_description = description of what each column in R is
    """

    n, m = u.shape

    if width_x == None: width_x = n//10
    if width_t == None: width_t = m//10
    if deg_t == None: deg_t = deg_x

    # If we're using polynomials to take derviatives, then we toss the data around the edges.
    if time_diff == 'poly':
        m2 = m-2*width_t
        offset_t = width_t
    else:
        m2 = m
        offset_t = 0
    if space_diff == 'poly':
        n2 = n-2*width_x
        offset_x = width_x
    else:
        n2 = n
        offset_x = 0

    if lam_t == None: lam_t = 1.0/m
    if lam_x == None: lam_x = 1.0/n

    ########################
    # First take the time derivaitve for the left hand side of the equation
    ########################
    ut = np.zeros((n2,m2), dtype=np.complex64)

    if time_diff == 'FDconv':
        Usmooth = np.zeros((n,m), dtype=np.complex64)
        # Smooth across x cross-sections
        for j in range(m):
            Usmooth[:,j] = ConvSmoother(u[:,j],width_t,sigma)
        # Now take finite differences
        for i in range(n2):
            ut[i,:] = FiniteDiff(Usmooth[i + offset_x,:],dt,1)

    elif time_diff == 'poly':
        T= np.linspace(0,(m-1)*dt,m)
        for i in range(n2):
            ut[i,:] = PolyDiff(u[i+offset_x,:],T,diff=1,width=width_t,deg=deg_t)[:,0]

    elif time_diff == 'Tik':
        for i in range(n2):
            ut[i,:] = TikhonovDiff(u[i + offset_x,:], dt, lam_t)

    else:
        for i in range(n2):
            ut[i,:] = FiniteDiff(u[i + offset_x,:],dt,1)

    ut = np.reshape(ut, (n2*m2,1), order='F')

    ########################
    # Now form the rhs one column at a time, and record what each one is
    ########################

    u2 = u[offset_x:n-offset_x,offset_t:m-offset_t]
    Theta = np.zeros((n2*m2, (D+1)*(P+1)), dtype=np.complex64)
    ux = np.zeros((n2,m2), dtype=np.complex64)
    rhs_description = ['' for i in range((D+1)*(P+1))]

    if space_diff == 'poly':
        Du = {}
        for i in range(m2):
            Du[i] = PolyDiff(u[:,i+offset_t],np.linspace(0,(n-1)*dx,n),diff=D,width=width_x,deg=deg_x)
    if space_diff == 'Fourier': ik = 1j*np.fft.fftfreq(n)*n

    for d in range(D+1):

        if d > 0:
            for i in range(m2):
                if space_diff == 'Tik': ux[:,i] = TikhonovDiff(u[:,i+offset_t], dx, lam_x, d=d)
                elif space_diff == 'FDconv':
                    Usmooth = ConvSmoother(u[:,i+offset_t],width_x,sigma)
                    ux[:,i] = FiniteDiff(Usmooth,dx,d)
                elif space_diff == 'FD': ux[:,i] = FiniteDiff(u[:,i+offset_t],dx,d)
                elif space_diff == 'poly': ux[:,i] = Du[i][:,d-1]
                elif space_diff == 'Fourier': ux[:,i] = np.fft.ifft(ik**d*np.fft.fft(ux[:,i]))
        else: ux = np.ones((n2,m2), dtype=np.complex64)

        for p in range(P+1):
            Theta[:, d*(P+1)+p] = np.reshape(np.multiply(ux, np.power(u2,p)), (n2*m2), order='F')

            if p == 1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u'
            elif p>1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u^' + str(p)
            if d > 0: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+\
                                                   'u_{' + ''.join(['x' for _ in range(d)]) + '}'

    return ut, Theta, rhs_description

def print_pde(w, rhs_description, ut = 'u_t'):
    pde = ut + ' = '
    first = True
    for i in range(len(w)):
        if w[i] != 0:
            if not first:
                pde = pde + ' + '
            pde = pde + "(%05f %+05fi)" % (w[i].real, w[i].imag) + rhs_description[i] + "\n   "
            first = False
    print(pde)





import re

def decompose_pde_terms(pde_list):
    results = []

    for pde in pde_list:
        if '=' not in pde:
            continue

        lhs, rhs = pde.split('=')
        lhs = lhs.strip()
        rhs = rhs.strip()

        # Split RHS into additive terms
        terms = [t.strip() for t in re.split(r'\s*\+\s*', rhs)]

        parsed_terms = []

        for term in terms:
            # print(term)
            # Further split additive term into multiplicative/divisive sub-terms
            sub_terms = re.split(r'\*|/', term.strip())
            i = 0

            for sub_term in sub_terms:

                sub_term = sub_term.strip()
                # print(sub_term)
                if not sub_term:
                    continue

                entry = {
                    'raw_term': sub_term,
                    'variable': None,
                    'power': 0,
                    'derivative_order': 0,
                    'derivative_variable': None,
                    'operation': None
                }

                # print(term)
                print(sub_term)
                # print(term == sub_term)
                # print(sub_terms)
                if term == sub_term:
                    entry['operation'] = '+'
                else:
                    positions = [match.start() for match in re.finditer(r'\*|/', term)]
                    print(positions)
                    if i < len(positions):
                        entry['operation'] = term[positions[i]]
                        i = i + 1
                        print(i)
                    else:
                        entry['operation'] = '+'







                # Check for 'u' based terms
                if 'u' in sub_term:
                    entry['variable'] = 'u'

                    # Power (u^n)
                    power_match = re.search(r'u\^(\d+)', sub_term)
                    if power_match:
                        entry['power'] = int(power_match.group(1))
                    else:
                        entry['power'] = 1

                    # Derivative part (e.g., u_xx, u_xxxx, u_t)
                    deriv_match = re.search(r'u_([xt]+)', sub_term)
                    if deriv_match:
                        deriv_str = deriv_match.group(1)
                        entry['derivative_order'] = len(deriv_str)
                        entry['derivative_variable'] = deriv_str[0]  # assume all same var, e.g., 'xxx'
                else:
                    entry['variable'] = sub_term
                    entry['power'] = 0

                parsed_terms.append(entry)

        results.append({
            'pde': pde,
            'terms': parsed_terms
        })

    return results



def library(P, D):
    """Generates operands and operators based on max Poly order P and Deriv order D."""
    operands = []
    operators = []
    for i in range(D + 1): operands.append("u" if i==0 else "u_" + "x" * i)
    operands.append("x")
    for j in range(2, P + 1): operators.append("^" + f"{j}")
    for op in ["/", "*", "+", "-"]: operators.append(op)
    print(f"Library(P={P}, D={D}) -> Operands: {operands}")
    print(f"Library(P={P}, D={D}) -> Operators: {operators}")
    return operands, operators

def pri_eq_split(eval_eq_list):

  priority_eq = []
  priority_eq_coef = []


  for i in eval_eq_list:

    expr = f"{i}"
    # expr = "u_xx + (-1.491555e-05)*u_xxxx + u*u_x"  # <- Try this too!

    # Extract expression without coefficients
    clean_expr = re.sub(r"\([+-]?\d*\.?\d+(?:e[+-]?\d+)?\)\*", "", expr)
    clean_expr = re.split(r"coef:\[", clean_expr)[0].strip()

    # Try to extract explicitly listed coefficients
    coefs_match = re.search(r"coef:\[([^\]]+)\]", expr)
    if coefs_match:
        coefs = [float(num.strip()) for num in coefs_match.group(1).split(',')]
    else:
        # If not present, extract from terms like (+...)* or (-...)*, including scientific notation
        coefs = re.findall(r"\([+-]?\d*\.?\d+(?:e[+-]?\d+)?\)", expr)
        coefs = [float(c.strip("()")) for c in coefs]

    priority_eq.append(clean_expr)
    priority_eq_coef.append(coefs)
  return priority_eq, priority_eq_coef





def pri_eq_split(eval_eq_list):

  priority_eq = []
  priority_eq_coef = []


  for i in eval_eq_list:

    expr = f"{i}"
    # expr = "u_xx + (-1.491555e-05)*u_xxxx + u*u_x"  # <- Try this too!

    # Extract expression without coefficients
    clean_expr = re.sub(r"\([+-]?\d*\.?\d+(?:e[+-]?\d+)?\)\*", "", expr)
    clean_expr = re.split(r"coef:\[", clean_expr)[0].strip()

    # Try to extract explicitly listed coefficients
    coefs_match = re.search(r"coef:\[([^\]]+)\]", expr)
    if coefs_match:
        coefs = [float(num.strip()) for num in coefs_match.group(1).split(',')]
    else:
        # If not present, extract from terms like (+...)* or (-...)*, including scientific notation
        coefs = re.findall(r"\([+-]?\d*\.?\d+(?:e[+-]?\d+)?\)", expr)
        coefs = [float(c.strip("()")) for c in coefs]

    priority_eq.append(clean_expr)
    priority_eq_coef.append(coefs)
  return priority_eq, priority_eq_coef


def self_improvement(json_str, operands, operators):
    prompt = f"""
Given the symbol library including operators: {operators} and operands:{operands}.
You will help me generate governing equations using symbolic representations.
I will evaluate the generated equations and provide their
corresponding scores based on their fitness to data. Your task is to find the equation
with the highest score.
Below are some previous equations and their scores, provided in json format which range from 0 to 1. The
equations are arranged in descending order based on their scores, where higher values
are better.
{json_str}

Motivated by the equations above, operators, and operands, please help me generate 10 new equations which will have higher scores,
and try to fit best coefficients based on your knowledge of PDEs.
Do permutation and combination within or between
the libraries given for forming mathematically and physically plausible PDE terms.
Do not always focus on exploitation (i.e., maximizing scores);
occasionally try exploration by including terms in the PDE that are physically
meaningful or theoretically justified, even if their contribution is not yet
confirmed.
Always generate PDEs that are mathematically and physically plausible.
Try to recognize and delete or remove redundant terms from PDEs.
Be creative, do not give the scores of the equation and do not give additional
explanations.
Do not give any extra text, just return PDEs without inverted commas and serial numbers.
"""
    response = llm.predict(prompt)
    candidate_features = [line.strip() for line in response.splitlines() if line.strip()]
    return candidate_features


def GA_by_llm(pri_eq, operands, operators):
    prompt = f"""
Given the symbol library including operators: {operators} and operands:{operands}
and following set of equations:
{pri_eq}
please follow the instructions step-by-step to generate new equations:
1. Select two different set of terms from above.
2. Crossover two set of terms chosen in step 1 and generate a new equation.
3. Mutate the equation generated in Step 2 and generate a new equation.
4. Give me the generated equation of step 3.
5. Try to fit best coefficients based on your knowledge of PDEs.
Crossover: Select half terms of each set and recombine the selected with '+' and '-'
to generate a new equation.
Mutate: Replace operators or operands of the equation only with new ones defined
in the symbol library.
Do not always focus on exploitation (i.e., given set of equations);
occasionally try exploration by including terms in the PDE that are physically
meaningful or theoretically justified, even if their contribution is not yet
confirmed.
Do permutation and combination within or between
the libraries given for forming mathematically and physically plausible PDE terms.
Always generate PDEs that are mathematically and physically plausible.
Try to recognize and delete or remove redundant terms from PDEs.
Be creative, do not give the scores of the equation and do not give additional
explanations.
Do not give any extra text, just return PDEs without inverted commas and serial numbers.
"""
    response = llm.predict(prompt)
    candidate_features = [line.strip() for line in response.splitlines() if line.strip()]
    return candidate_features


# si
import re

def extract_all_expressions(lines):
    expressions = []
    buffer = []
    inside_tag = False

    for line in lines:
        line = line.strip()

        # Handle one-line tag format: <tag>expression</tag>
        match = re.search(r'<[^>]+>([^<]+)</[^>]+>', line)
        if match:
            expressions.append(match.group(1).strip())
            continue

        # Detect start of multi-line tag
        if re.match(r'<[^/][^>]*>', line):
            inside_tag = True
            buffer = []
            continue

        # Detect end of multi-line tag
        if re.match(r'</[^>]+>', line):
            inside_tag = False
            if buffer:
                expressions.append(' '.join(buffer).strip())
                buffer = []
            continue

        # Collect lines between tags
        if inside_tag:
            buffer.append(line)

    return expressions





# --- Finite Difference Helper Functions ---
def FiniteDiff(u, dx, d):
    n=u.size; ux=np.zeros(n,dtype=float)
    if d==0: return u
    min_points=max(9 if d>=4 else 7 if d>=3 else 5 if d>=2 else 3 if d>=1 else 1, 1)
    if n < min_points: raise ValueError(f"Need >={min_points} points for d={d} FD, got {n}")
    if d==1: ux[1:-1]=(u[2:]-u[:-2])/(2*dx); ux[0]=(-3./2*u[0]+2*u[1]-u[2]/2)/dx; ux[n-1]=(3./2*u[n-1]-2*u[n-2]+u[n-3]/2)/dx
    elif d==2: ux[1:-1]=(u[2:]-2*u[1:-1]+u[:-2])/dx**2; ux[0]=(2*u[0]-5*u[1]+4*u[2]-u[3])/dx**2; ux[n-1]=(2*u[n-1]-5*u[n-2]+4*u[n-3]-u[n-4])/dx**2
    elif d==3:
        for i in range(2,n-2): ux[i]=(u[i+2]/2-u[i+1]+u[i-1]-u[i-2]/2)/dx**3
        ux[0]=(-2.5*u[0]+9*u[1]-12*u[2]+7*u[3]-1.5*u[4])/dx**3; ux[1]=(-2.5*u[1]+9*u[2]-12*u[3]+7*u[4]-1.5*u[5])/dx**3
        ux[n-1]=(2.5*u[n-1]-9*u[n-2]+12*u[n-3]-7*u[n-4]+1.5*u[n-5])/dx**3; ux[n-2]=(2.5*u[n-2]-9*u[n-3]+12*u[n-4]-7*u[n-5]+1.5*u[n-6])/dx**3
    elif d==4:
        try: uxx = FiniteDiff(u, dx, 2); uxxxx = FiniteDiff(uxx, dx, 2); ux = uxxxx
        except ValueError as e: raise ValueError(f"Recursive FD failed for d=4: {e}")
    elif d > 4: raise ValueError(f"FiniteDiff order d={d} not implemented beyond d=4.")
    return ux

def finite_diff_2d_wrapper(data, axis, diff_order, dx):
    if data.ndim != 2: raise ValueError("Input data must be 2D")
    output = np.zeros_like(data, dtype=float);
    try:
        if axis == 0: # Space axis
            if diff_order == 0: return data.copy()
            for j in range(data.shape[1]): output[:, j] = FiniteDiff(data[:, j], dx, diff_order)
        elif axis == 1: # Time axis
            if diff_order == 0: return data.copy()
            for i in range(data.shape[0]): output[i, :] = FiniteDiff(data[i, :], dx, diff_order)
        else: raise ValueError("Axis must be 0 or 1")
        if np.isnan(output).any() or np.isinf(output).any():
            print(f"    WARNING: NaN/Inf generated during FD (axis={axis}, order={diff_order}). Replacing with 0.")
            np.nan_to_num(output, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
    except ValueError as e: print(f"    ERROR in FiniteDiff (axis={axis}, order={diff_order}): {e}"); raise
    return output

# --- Term Evaluation (Direct Calculation) ---
term_cache = {}
def evaluate_term_direct(term_name: str, u_data: np.ndarray, x_grid_1d: np.ndarray, dx: float, dt: float) -> Optional[np.ndarray]:
    """ Calculates the numerical value of a term string using u_data, x_grid. """
    global term_cache
    # Ensure term_name is base name (no leading sign)
    term_name = term_name.strip().lstrip('+-')
    if not term_name: return None # Skip if only sign was passed

    if term_name in term_cache: return term_cache[term_name]
    # print(f"      Calculating '{term_name}'...") # Keep commented unless deep debug needed
    result = None; original_shape = u_data.shape
    try:
        if term_name == '1': result = np.ones_like(u_data, dtype=float)
        elif term_name == 'u': result = u_data.copy().astype(float)
        elif term_name == 'x':
            if x_grid_1d.shape[0] != u_data.shape[0]: raise ValueError("x_grid shape mismatch")
            result = np.tile(x_grid_1d.reshape(-1, 1), (1, u_data.shape[1])).astype(float)
        elif '^' in term_name:
            match = re.match(r"(.+)\^(\d+)", term_name)
            if match:
                base, exponent_str = match.groups(); exponent = int(exponent_str)
                base_field = evaluate_term_direct(base, u_data, x_grid_1d, dx, dt)
                if base_field is None: return None
                result = np.power(base_field, exponent)
            else: raise ValueError(f"Invalid power format '{term_name}'")
        elif '*' in term_name:
            factors = term_name.split('*');
            result_accum = None
            for i, factor in enumerate(factors):
                factor_field = evaluate_term_direct(factor, u_data, x_grid_1d, dx, dt)
                if factor_field is None: return None
                result_accum = factor_field.copy() if i == 0 else result_accum * factor_field
            result = result_accum
        elif '/' in term_name:
            parts = term_name.split('/', 1);
            if len(parts) == 2:
                num_name, den_name = parts;
                num_field = evaluate_term_direct(num_name, u_data, x_grid_1d, dx, dt)
                den_field = evaluate_term_direct(den_name, u_data, x_grid_1d, dx, dt)
                if num_field is None or den_field is None: return None
                den_abs_min = np.min(np.abs(den_field));
                if den_abs_min < 1e-9: print(f"        WARNING TermEval: Denominator near zero for '{term_name}'.")
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", category=RuntimeWarning); result = num_field / (den_field + 1e-9 * np.sign(den_field + 1e-15))
            else: raise ValueError(f"Invalid division format '{term_name}'")
        elif term_name.startswith('u_'): # Derivatives
            deriv_part = term_name[2:]; diff_order = len(deriv_part); axis = 0; step = dx
            if diff_order > 0 and all(c == 'x' for c in deriv_part):
                base_u = evaluate_term_direct('u', u_data, x_grid_1d, dx, dt)
                if base_u is None: return None
                result = finite_diff_2d_wrapper(base_u, axis=axis, diff_order=diff_order, dx=step)
            else: raise ValueError(f"Unknown derivative pattern '{term_name}'")
        else: raise ValueError(f"Cannot parse/eval term '{term_name}'")

        # --- Post-calculation Checks ---
        if result is None: print(f"      DEBUG TermEval: Result is None for '{term_name}'")
        elif result.shape != original_shape: print(f"      ERROR TermEval: Shape mismatch! '{term_name}' shape {result.shape}, expected {original_shape}"); return None
        else:
            nan_count = np.sum(np.isnan(result)); inf_count = np.sum(np.isinf(result))
            if nan_count > 0 or inf_count > 0: print(f"      WARNING TermEval: Result for '{term_name}' contains {nan_count} NaN / {inf_count} Inf. Returning None."); return None
            else: term_cache[term_name] = result
        return result
    except ValueError as ve: print(f"      Error evaluating term '{term_name}': {ve}"); return None
    except Exception as e: print(f"      Unexpected error evaluating term '{term_name}': {e}"); traceback.print_exc(); return None

# --- EDL Utility ---
def replace_consts(input_string):
    count = 0
    def repl(match): nonlocal count; replacement = f"c_{count}"; count += 1; return replacement
    result_string = re.sub(r'\bconst\b', repl, input_string)
    return input_string, count

# --- EDL Program Class ---
class Program:
    # Using definition provided by user
    # def __init__(self, expr, lhs, features):
    #     # self.optimizer = ScipyMinimize()
    #     self.expr = expr; exp_str,count = replace_consts(expr)
    #     try: self.eqs_sympy = sp.sympify(exp_str)
    #     except (SyntaxError, TypeError, sp.SympifyError) as e: raise ValueError(f"Sympy parsing failed for '{exp_str}': {e}") from e
    #     self.const_symbols = sp.symbols([f'c_{i}' for i in range(count)])
    #     self.feature_names = list(features.keys()); self.var_symbols = sp.symbols(self.feature_names)
    #     self.lhs = lhs.reshape(-1); self.y_rhs = [features[name] for name in self.feature_names]
    #     self.init_const = [ random.uniform(-1, 1) for _ in range(count)]

    def __init__(self, expr, lhs, features):
        # self.optimizer = ScipyMinimize()
        self.expr = expr

        try:
            self.eqs_sympy = sp.sympify(expr)
        except (SyntaxError, TypeError, sp.SympifyError) as e:
            raise ValueError(f"Sympy parsing failed for '{expr}': {e}") from e

        # Extract symbols from the expression
        all_symbols = self.eqs_sympy.free_symbols

        # Feature variable symbols (input variables)
        self.feature_names = list(features.keys())
        self.var_symbols = sp.symbols(self.feature_names)

        # Constants = all symbols - variable symbols
        self.const_symbols = list(all_symbols - set(self.var_symbols))


        # Setup LHS and RHS
        self.lhs = lhs.reshape(-1)
        self.y_rhs = [features[name] for name in self.feature_names]


    def loss(self,rhs):
        if rhs is None or not isinstance(rhs, np.ndarray) or np.any(np.isnan(rhs)) or np.any(np.isinf(rhs)): return 1e10
        try: loss = np.mean(np.square(rhs-self.lhs))
        except ValueError: return 1e10 # Shape mismatch
        return loss if not (np.isnan(loss) or np.isinf(loss)) else 1e10

    def process_sym(self, consts):
        try:
            const_subs = dict(zip(self.const_symbols, consts)); eq_subs = self.eqs_sympy.subs(const_subs)
            f = sp.lambdify(self.var_symbols, eq_subs, ['numpy', {'conjugate': lambda x: x}])
            rhs =f(*self.y_rhs); loss = self.loss(rhs)
        except Exception: loss = 1e10
        return loss

    def rhs_evaluate(self,consts,y_rhs_features):
        rhs = np.zeros_like(self.lhs)
        try:
            const_subs = dict(zip(self.const_symbols, consts)); eq_subs = self.eqs_sympy.subs(const_subs)
            f = sp.lambdify(self.var_symbols, eq_subs, ['numpy', {'conjugate': lambda x: x}])
            rhs =f(*y_rhs_features)
            if np.any(np.isnan(rhs)) or np.any(np.isinf(rhs)): print(f"    WARNING: NaN/Inf in rhs_evaluate."); rhs[:] = 0
        except Exception as e: print(f"    ERROR: rhs_evaluate Failed: {e}")
        return rhs

    def optimize_constants(self,):
        consts=np.array(self.const_symbols)
        print(consts)
        # if len(self.init_const)>0:
        #     try:
        #          bounds = [(-10, 10)] * len(self.init_const)
        #          if not callable(self.optimizer): raise TypeError("Optimizer not callable")
        #          optim_result = self.optimizer(self.process_sym, self.init_const, bounds=bounds)
        #          if isinstance(optim_result, np.ndarray): consts = optim_result
        #          elif hasattr(optim_result, 'x'): consts = optim_result.x
        #          else: raise TypeError(f"Unexpected optimizer result: {type(optim_result)}")
        #          if np.any(np.isnan(consts)) or np.any(np.isinf(consts)): print("    WARNING: Optimizer returned NaN/Inf."); consts = np.array(self.init_const)
        #     except Exception as e: print(f"    WARNING: Optimizer failed: {e}."); consts = np.array(self.init_const)
        return consts

# --- EDL Evaluator Class (Using Custom Linear Fit) ---
class Evaluator:
    cache = {}
    def __init__(self, data_name, metric='inv_nrmse', metric_params=[], top_k=5, max_terms=10, l0_penalty=1e-5, mode='custom_lstsq', add_const=0, noise=0):
        self.data_name = data_name; self.lhs = None; self.feature_dict = None; self.train_rhs = None; self.test_rhs = None; self.y_test = None; self.lhs_test = None; self.feature_names = []; self.features = []
        self.max_terms = max_terms; self.l0_penalty = l0_penalty; self.mode = mode;
        if not isinstance(metric_params, (list, tuple)): metric_params = [metric_params]
        try: self.metrics = make_metric(metric, *metric_params); print(f"Evaluator initialized with metric: {metric}")
        except NameError: print("CRITICAL ERROR: 'make_metric' not defined."); sys.exit(1)
        except AssertionError as ae:
             print(f"ERROR creating metric '{metric}': {ae}.")
             fallback_metric = 'R2'; print(f"Defaulting to '{fallback_metric}' metric.")
             try: self.metrics = make_metric(fallback_metric); self.metrics.__name__ = fallback_metric
             except Exception as fallback_e:
                  print(f"ERROR: Fallback metric '{fallback_metric}' failed: {fallback_e}");
                  def mse_fallback(y_true, y_pred, complexity=0):
                      if y_pred is None or not isinstance(y_pred, np.ndarray) or np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)): return float('inf')
                      if y_true.shape != y_pred.shape: y_pred = np.resize(y_pred, y_true.shape)
                      mse = np.mean(np.square(y_true - y_pred)); return mse if not (np.isnan(mse) or np.isinf(mse)) else float('inf')
                  self.metrics = mse_fallback; self.metrics.__name__ = "MSE_Fallback"; print("Using basic MSE calculation.")
        except Exception as e: print(f"ERROR creating metric '{metric}': {e}."); sys.exit(1)
        self.invalid = {}; self.pq = PriorityQueue(top_k); self.add_const = add_const

    def set_data(self, lhs_vector, feature_dictionary, train_lhs=None, test_lhs=None, test_features_dict=None):
         self.lhs = lhs_vector; self.feature_dict = feature_dictionary; self.feature_names = list(self.feature_dict.keys()); self.features = [self.feature_dict[name] for name in self.feature_names]
         self.train_rhs = train_lhs if train_lhs is not None else self.lhs; self.lhs_test = test_lhs
         if test_features_dict: self.y_test = [test_features_dict[name] for name in self.feature_names]
         else: self.y_test = None
         self.test_rhs = test_lhs
         print("Evaluator data set manually:"); print(f"  LHS shape: {self.lhs.shape if self.lhs is not None else 'None'}"); print(f"  Feature names: {self.feature_names}"); print(f"  Number of features: {len(self.features)}")

    def evaluate_score_nonlinear(self,eq_list):
        # Unchanged from previous version - uses Program class
        equations = []; eq_scores = {}; invalid_reasons = {}
        nonlinear_feature_names = list(self.feature_names) + ['const']; symbols = create_sympy_symbols(nonlinear_feature_names)
        print(f"\n  Evaluating {len(eq_list)} NONLINEAR candidates...");
        if self.lhs is None or self.feature_dict is None: print("  ERROR: Data not set."); return invalid_reasons, []
        for i, eq_str_orig in enumerate(eq_list):
            eq = eq_str_orig.strip(); score = float('inf'); optim_coefs=None; eq_sympy=None; len_ori=0; extra_metric={}; y_rhs_train=None
            if not eq: continue; print(f"    Nonlinear candidate {i+1}: {eq}")
            try:
                eq_sympy = str2sympy(eq, nonlinear_feature_names)
                string_error = check_symbols_valid(eq_sympy, symbols)
                if string_error : print(f"      Invalid symbol: {string_error}"); invalid_reasons[string_error] = 1+invalid_reasons.get(string_error,0); continue
                func_strs = count_functerms( eq_sympy); len_ori = len(func_strs)
                if len(func_strs)>10: print("      Skipping: Too many terms."); continue
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore"); p_eq = Program( eq, self.lhs, self.feature_dict)
                    if len(p_eq.init_const)>6: print("      Skipping: Too many constants."); continue
                    optim_coefs = p_eq.optimize_constants(); y_rhs_train = p_eq.rhs_evaluate(optim_coefs, self.features)
                score = self.metrics(self.lhs, y_rhs_train, len(func_strs))
                if np.isnan(score) or np.isinf(score): print("      Skipping: Score is NaN/Inf."); continue
                try: r2_train = r2_score(self.train_rhs, y_rhs_train); extra_metric['r2_train'] = r2_train
                except Exception as r2e: print(f"      Warning: R2 train failed: {r2e}"); extra_metric['r2_train'] = float('nan')
                if self.y_test is not None and self.test_rhs is not None:
                    try: y_rhs_test = p_eq.rhs_evaluate(optim_coefs, self.y_test); r2_test = r2_score(self.test_rhs, y_rhs_test); extra_metric['r2_test'] = r2_test
                    except Exception as r2etest: print(f"      Warning: R2 test failed: {r2etest}"); extra_metric['r2_test'] = float('nan')
            except ValueError as e: print(f"      Skipping (ValueError): {e}"); invalid_reasons[f"ValueError: {e}"] = 1+invalid_reasons.get(f"ValueError: {e}",0); continue
            except Exception as e2: print(f"      Skipping (Other Error): {e2}"); invalid_reasons[f"Exception: {e2}"] = 1+invalid_reasons.get(f"Exception: {e2}",0); continue
            if eq not in eq_scores: eq_scores[eq] = round(score, 5); equation = Equation(eq, round(score,5), optim_coefs, eq_sympy, len_ori, extra_metric); equations.append(equation)
        self.invalid = merge_dict(self.invalid, invalid_reasons); return invalid_reasons, equations

    # # --- evaluate_score_linear (REWRITTEN with fixes for parsing/evaluation) ---
    def evaluate_score_linear(self, eq_list):
        eq_scores = {}; invalid_reasons = {}; equations = []
        symbols = create_sympy_symbols(self.feature_names) # Base symbols for parsing
        # print(f"\n  Evaluating {len(eq_list)} LINEAR candidates (using CUSTOM direct lstsq)...");
        if self.lhs is None or self.feature_dict is None: print("  ERROR: Data not set."); return invalid_reasons, []
        expected_len = self.lhs.shape[0]
        lhs_1d = self.lhs.reshape(-1)

        for i, eq_str_orig in enumerate(eq_list):
            eq = eq_str_orig.strip(); score=float('inf'); final_coef=None; eq_sympy_final=None; len_ori=0; extra_metrics={}; y_rhs=None; final_eq_str=eq; valid=True; base_term_names=[]
            if not eq: continue; print(f"    Linear candidate {i+1}: {eq}")
            global term_cache; term_cache.clear()

            processed_func_terms = [] # Holds numerical data arrays
            func_terms_names = [] # Holds string names of base terms

            try:
                # --- Sympy Conversion ---
                eq_sympy_parsed = str2sympy(eq, self.feature_names) # Parse original string

                # *** Check Type AFTER str2sympy ***
                if not isinstance(eq_sympy_parsed, sp.Expr):
                     print(f"      ERROR: str2sympy did not return valid sympy expr (got {type(eq_sympy_parsed)}). Skipping.")
                     invalid_reasons[f"SympyType Error in {eq}"] = 1+invalid_reasons.get(f"SympyType Error in {eq}",0); continue

                # --- Term Extraction and Evaluation ---
                # Get terms, separating coefficients (like -1)
                if isinstance(eq_sympy_parsed, sp.Add): terms_sympy_with_coeffs = list(sp.Add.make_args(eq_sympy_parsed))
                elif isinstance(eq_sympy_parsed, (sp.Symbol, sp.Mul, sp.Pow, sp.Function, sp.Number, sp.Rational, sp.Float)): terms_sympy_with_coeffs = [eq_sympy_parsed]
                else: print(f"      Warning: Unexpected sympy type {type(eq_sympy_parsed)}. Skipping."); invalid_reasons[f"Unexpected Sympy Type in {eq}"] = 1+invalid_reasons.get(f"Unexpected Sympy Type in {eq}",0); continue

                valid_terms_check = True
                len_ori = len(terms_sympy_with_coeffs) # Original number of Add args

                term_dict_for_fit = {} # Store {base_term_name: term_data}
                coeff_original = []
                for k, sub_expr in enumerate(terms_sympy_with_coeffs):
                    # Separate numerical coefficient (like +1, -1) from the base term
                    coeff_part, term_part = sub_expr.as_coeff_Mul() # Returns (coeff, rest)
                    # print(coeff_part)
                    coeff_original.append(float(coeff_part))
                    # coeff_original.append(coeff_part)
                    # coeff_original = np.array(coeff_original)
                    # print(coeff_original)
                    # print(type(coeff_original))
                    # print(term_part)
                    term_name_base = str(term_part).replace('**','^') # Get base term name string

                    # print(f"      Term {k+1}: Expr='{sub_expr}', Coeff={coeff_part}, Base='{term_name_base}'") # Debug

                    # Evaluate the BASE term using evaluate_term_direct
                    term_data = evaluate_term_direct(term_name_base, u_data, x_flat, dx, dt)

                    if term_data is None: print(f"      ERROR: evaluate_term_direct failed for base term '{term_name_base}'. Skipping PDE."); valid_terms_check = False; break
                    term_data_flat = term_data.flatten()
                    if term_data_flat.shape[0] != expected_len: print(f"      ERROR: Term '{term_name_base}' shape mismatch. Skipping."); valid_terms_check = False; break
                    if np.isnan(term_data_flat).any() or np.isinf(term_data_flat).any(): print(f"      ERROR: Term '{term_name_base}' contains NaN/Inf. Skipping."); valid_terms_check = False; break

                    # Store data associated with the base term name
                    # If base term already seen (e.g., u_xx + 2*u_xx), lstsq will handle it
                    if term_name_base not in term_dict_for_fit:
                         term_dict_for_fit[term_name_base] = term_data_flat
                         func_terms_names.append(term_name_base) # Keep track of unique base terms evaluated
                    #
                coeff_original = np.array(coeff_original)
                # print(coeff_original)
                # print(type(coeff_original))
                if not valid_terms_check: invalid_reasons[f"Term Eval/Shape/NaN Error in {eq}"] = 1+invalid_reasons.get(f"Term Eval/Shape/NaN Error in {eq}",0); continue

                # Get the data arrays in the correct order for the matrix
                processed_func_terms = [term_dict_for_fit[name] for name in func_terms_names]

                if not processed_func_terms: print("      Skipping: No valid terms evaluated."); continue

                # --- Remove Redundants (on evaluated data) ---
                # This check might be less necessary now, but keep for robustness
                processed_func_terms, func_terms_names, duplicates = remove_redundants(processed_func_terms, func_terms_names)
                if not processed_func_terms: print("      Skipping: No terms left after removing redundants."); continue

                # --- Direct Least Squares Fit ---
                # print(f"      Performing direct least squares with {len(processed_func_terms)} unique base terms...")
                rhs_matrix = np.array(processed_func_terms).T
                # print(f"      DEBUG: Assembled rhs matrix shape: {rhs_matrix.shape}") # Optional
                # print(f"      DEBUG: lhs vector shape: {lhs_1d.shape}") # Optional

                if rhs_matrix.shape[0] != lhs_1d.shape[0]: print(f"      ERROR: Shape mismatch RHS ({rhs_matrix.shape[0]}) vs LHS ({lhs_1d.shape[0]}). Skipping."); invalid_reasons[f"LHS/RHS Mismatch in {eq}"] = 1+invalid_reasons.get(f"LHS/RHS Mismatch in {eq}",0); continue

                try:
                     coef, residuals, rank, s = np.linalg.lstsq(rhs_matrix, lhs_1d, rcond=None)
                    #  coef = coeff_original
                    #  print(coef)
                    #  print(type(coef))
                     valid = True; error_type = None
                except np.linalg.LinAlgError as lae: print(f"      Linear fit failed (LinAlgError): {lae}"); valid = False; error_type = f"LinAlgError: {lae}"; invalid_reasons[error_type] = 1+invalid_reasons.get(error_type,0); continue
                except ValueError as ve: print(f"      Linear fit failed (ValueError): {ve}"); valid = False; error_type = f"ValueError: {ve}"; invalid_reasons[error_type] = 1+invalid_reasons.get(error_type,0); continue

                # --- Pruning & Storing Results ---
                active_indices = [k for k, c in enumerate(coef) if abs(c) > 1e-7][:self.max_terms]
                if not active_indices: print("      Skipping: All coefficients zero after lstsq."); continue

                final_func_strs = [func_terms_names[k] for k in active_indices]; # Names of active terms
                final_coef = [coef[k] for k in active_indices] # Coeffs of active terms
                active_func_terms_data = [processed_func_terms[k] for k in active_indices] # Data of active terms

                if active_func_terms_data: y_rhs = np.sum([cf * tm for cf, tm in zip(final_coef, active_func_terms_data)], axis=0)
                else: y_rhs = np.zeros_like(lhs_1d)

                # Reconstruct final equation string and sympy expression from fitted terms/coeffs
                try:
                    final_eq_str = " + ".join([f"({cf:+.7g})*{name}" for cf, name in zip(final_coef, final_func_strs)]).replace('+ -','- ')
                    # Parse the *fitted* equation string to get a final sympy representation
                    eq_sympy_final = str2sympy(final_eq_str, self.feature_names)
                    if not isinstance(eq_sympy_final, sp.Expr): eq_sympy_final = eq_sympy_parsed # Fallback
                except Exception as reorg_e: print(f"      Warning: Reconstructing final eq string/sympy failed: {reorg_e}.") ; final_eq_str = eq; eq_sympy_final = eq_sympy_parsed # Fallback

            # Catch errors from initial parsing or term evaluation
            except SyntaxError as Se: print(f"      Skipping (Syntax Error): {Se}\n      Problem string: '{eq}'"); invalid_reasons[f"SyntaxError: {Se}"] = 1+invalid_reasons.get(f"SyntaxError: {Se}",0); continue
            except (TypeError, ValueError, AttributeError, RecursionError) as parse_eval_e: print(f"      Skipping (Error during parsing/term eval): {parse_eval_e}"); invalid_reasons[f"{type(parse_eval_e).__name__}: {parse_eval_e}"] = 1+invalid_reasons.get(f"{type(parse_eval_e).__name__}: {parse_eval_e}",0); continue
            except Exception as e: print(f"      Skipping (Unexpected Error): {e}"); invalid_reasons[f"Exception: {e}"] = 1+invalid_reasons.get(f"Exception: {e}",0); continue

            # --- Scoring & Metrics ---
            if valid and y_rhs is not None:
                try:
                    score = self.metrics(lhs_1d, y_rhs, len(final_func_strs)) # Score based on N fitted terms
                    if np.isnan(score) or np.isinf(score): print("      Skipping: Score is NaN/Inf."); continue
                    try: r2_train = r2_score(self.train_rhs.reshape(-1), y_rhs); extra_metrics['r2_train'] = r2_train
                    except Exception as r2e: print(f"      Warning: R2 train failed: {r2e}"); extra_metrics['r2_train'] = float('nan')
                    # Test set evaluation would need modification if needed

                    # Store results using final fitted equation string and coefficients
                    if final_eq_str not in eq_scores: eq_scores[final_eq_str] = round(score,5); equation = Equation(final_eq_str, round(score,5), final_coef, eq_sympy_final, len_ori, extra_metrics); equations.append(equation)
                except Exception as score_e: print(f"      Skipping (Error during scoring/metrics): {score_e}"); invalid_reasons[f"Scoring Error: {score_e}"] = 1+invalid_reasons.get(f"Scoring Error: {score_e}",0)

        self.invalid = merge_dict(self.invalid, invalid_reasons); return invalid_reasons, equations



    import numpy as np
    from sympy import symbols, Eq, sympify, Function, Derivative, lambdify

    def equation_to_matrix(eq_str, func_name='u', sp_symbols=('x', 'y')):
        x, y = symbols(sp_symbols)
        eq = Eq(*map(sympify, eq_str.split('=')))

        lhs = eq.lhs - eq.rhs
        lhs = lhs.expand()

        func_terms_names = []
        coeff_part_dict = {}
        func_terms = []

        for sub_expr in lhs.args if lhs.is_Add else [lhs]:
            coeff_part, term_part = sub_expr.as_coeff_Mul()
            if isinstance(term_part, Derivative):
                var_tuple = tuple(term_part.variables)
                deriv_order = tuple(var_tuple.count(var) for var in (x, y))
                term_name_base = f"{func_name}_" + "x" * deriv_order[0] + "y" * deriv_order[1]
                func_terms_names.append(term_name_base)
                coeff_part_dict[term_name_base] = float(coeff_part)
                func_terms.append(term_part)
            elif term_part == Function(func_name)(x, y):
                term_name_base = func_name
                func_terms_names.append(term_name_base)
                coeff_part_dict[term_name_base] = float(coeff_part)
                func_terms.append(term_part)
            elif term_part.free_symbols <= {x, y}:  # constant term
                term_name_base = '1'
                func_terms_names.append(term_name_base)
                coeff_part_dict[term_name_base] = float(coeff_part)
                func_terms.append(sympify(1))

        seen = set()
        func_terms_unique = []
        func_terms_names_unique = []
        for name, term in zip(func_terms_names, func_terms):
            if name not in seen:
                seen.add(name)
                func_terms_unique.append(term)
                func_terms_names_unique.append(name)

        func_lambdas = [lambdify((x, y), term, 'numpy') for term in func_terms_unique]

        def matrix_func(x_val, y_val):
            return np.array([f(x_val, y_val) for f in func_lambdas])

        return matrix_func, func_terms_names_unique, [coeff_part_dict[name] for name in func_terms_names_unique]


    # def evaluate_score_linear(eq_str, x_data, y_data, u_data):
    #     matrix_func, var_names, coeffs = equation_to_matrix(eq_str)
    #     n_points = len(x_data)

    #     # X = np.zeros((n_points, len(var_names)))
    #     # for i in range(n_points):
    #     #     X[i, :] = matrix_func(x_data[i], y_data[i])

    #     # Compute prediction using retained coeffs directly
    #     coeffs = np.array(coeffs)
    #     pred = X @ coeffs

    #     score = np.linalg.norm(pred - u_data) / np.linalg.norm(u_data)
    #     return score, dict(zip(var_names, coeffs))

    # --- evaluate_score ---
    def evaluate_score(self,llm_out):
        eq_list = self.preprocess(llm_out); eq_linear,eq_nonlinear = [],[]
        for eq in eq_list:
            if len(eq)<1: continue
            if 'const' in eq or re.search(r'\bc_\d+\b', eq): eq_nonlinear.append(eq)
            else: eq_linear.append(eq)
        _, eqs_linear = self.evaluate_score_linear(eq_linear) # Calls NEW linear evaluator
        _, eqs_nonlinear = self.evaluate_score_nonlinear(eq_nonlinear) # Calls original nonlinear evaluator
        eqs = eqs_linear + eqs_nonlinear; return len(eqs_nonlinear), eqs

    # --- preprocess (Added cleaning for trailing chars) ---
    def preprocess(self, llm_out):
        print("--- Preprocessing LLM Output ---")
        processed_list = []; eq_list_raw = []
        if isinstance(llm_out, str):
            if '<res>' in llm_out and '</res>' in llm_out:
                 print("  Extracting equations using <res> tags...");
                 try: eq_list_raw = extract_eqs(llm_out)
                 except NameError: print("  Warning: extract_eqs not found."); eq_list_raw = llm_out.split('\n')
            else: eq_list_raw = llm_out.split('\n')
        elif isinstance(llm_out, list): eq_list_raw = llm_out
        else: print("  Warning: Unexpected input type for preprocessing."); return []
        standard_eqs = []
        for i, eq in enumerate(eq_list_raw):
            expression = eq.strip();
            if not expression: continue
            expression_cleaned = expression;
            try:
                expression_cleaned = re.sub(r"^\s*\d+[\.:\)]?\s*", "", expression_cleaned)
                expression_cleaned = re.sub(r"^\s*[A-Za-z\s\-]+\s*[:\-]\s*", "", expression_cleaned)
                expression_cleaned = re.sub(r"^[Uu]_?t\s*=\s*", "", expression_cleaned).strip()
                expression_cleaned = expression_cleaned.replace('u*(1-u)', 'u-u^2')
                # *** ADDED CLEANING for trailing chars ***
                expression_cleaned = expression_cleaned.rstrip(',\'"')
                if expression_cleaned: print(f"  Cleaned Eq {i+1} RHS: {expression_cleaned}"); standard_eqs.append(expression_cleaned)
                else: print(f"  Skipping empty Eq {i+1} after cleaning: {expression}")
            except Exception as e: print(f"  Error preprocessing line: {eq}\n    Error: {e}")
        print(f"  Preprocessing resulted in {len(standard_eqs)} candidate RHS expressions.")
        return standard_eqs

# extract_all_expressions(si)

def load_pde_data_and_subsample(dataset_name, n_x_sub=7, n_t_sub=5):

    import scipy.io as scio
    """
    Loads u, x, t data for a specified dataset using predefined paths
    and variable names, then generates a formatted text string
    representing a raw subsample of u(x, t).
    """
    u, x, t = None, None, None # Initialize
    subsample_text = f"Subsample could not be generated for dataset: {dataset_name}" # Default error

    print(f"\n--- Loading data for dataset: {dataset_name} ---")

    try:
        # --- Determine file paths and variable names based on dataset_name ---
        # Using paths and variable names from user's provided data_load function
        if dataset_name == 'chafee-infante':
            base_path = "/content"
            u = np.load(os.path.join(base_path, "chafee_infante_CI.npy"))
            x = np.load(os.path.join(base_path, "chafee_infante_x.npy")).reshape(-1,1)
            t = np.load(os.path.join(base_path, "chafee_infante_t.npy")).reshape(-1,1)

        elif dataset_name == 'Burgers':
            # Using path from user's function definition for Burgers
            fpath = os.path.join('/','content', 'burgers.mat')
            print(f"Loading 'Burgers' data from: {fpath}")
            if not os.path.exists(fpath): raise FileNotFoundError(f"File not found: '{fpath}'")
            data = scio.loadmat(fpath)
            u=data.get("usol")
            x=np.squeeze(data.get("x")).reshape(-1,1)
            t_raw=data.get("t")
            t=np.squeeze(t_raw).reshape(-1,1) if t_raw is not None else None

        elif dataset_name == 'PDE_divide':
            fpath = os.path.join('content', 'PDE_divide.npy')
            u_full=np.load(fpath).T # Load before slicing
            nx_orig, nt_orig = 100, 251
            x_full=np.linspace(1,2,nx_orig).reshape(-1,1)
            t_full=np.linspace(0,1,nt_orig).reshape(-1,1)
            slice_idx = slice(3, -3) # Define slice
            # Apply slice
            u = u_full[slice_idx, slice_idx]
            x = x_full[slice_idx]
            t = t_full[slice_idx]

        elif dataset_name == 'KS':
            # --- Use specific path for KS dataset as requested ---
            ks_file_path = '/content/kuramoto_sivishinky.mat'
            print(f"Loading 'KS' data from specified path: {ks_file_path}")
            if not os.path.exists(ks_file_path):
                 # Try fallback relative path if specific one not found? Or just error? Let's error.
                 raise FileNotFoundError(f"Specified KS file not found: '{ks_file_path}'")
            # Use scipy.io and variable names from user's KS block in the previous version
            import scipy.io # Ensure scipy.io is imported for this block
            data = scipy.io.loadmat(ks_file_path)
            # Variable names u, x, t as per user's function snippet for KS
            u = data['uu']
            x = data['x'][:,0]
            t = data['tt'][0,:]
            dt = t[1]-t[0]
            dx = x[2]-x[1]
            # --- End specific path and user's KS loading logic ---

        elif dataset_name == 'KS2':
            # Check path carefully, seems to point to same base file as user's KS?
            fpath = os.path.join('content', 'KS.mat')
            print(f"Loading 'KS2' data from: {fpath}")
            if not os.path.exists(fpath): raise FileNotFoundError(f"File not found: '{fpath}'")
            import scipy.io # Ensure imported
            data = scipy.io.loadmat(fpath)
            t = np.real(data['t'].flatten()[:,None])
            x = np.real(data['x'].flatten()[:,None])
            u = np.real(data['usol']) # Uses 'usol'

        elif dataset_name == "fisher":
            fpath = os.path.join('content',  'fisher_nonlin_groundtruth.mat')
            print(f"Loading 'fisher' data from: {fpath}")
            if not os.path.exists(fpath): raise FileNotFoundError(f"File not found: '{fpath}'")
            import scipy.io # Ensure imported
            data=scipy.io.loadmat(fpath)
            x_full=np.squeeze(data['x']).reshape(-1,1)
            t_full=np.squeeze(data['t']).reshape(-1,1)
            u_full=data['U'].T
            slice1=slice(1,-1); slice2=slice(3,-3)
            x=x_full[slice1][slice2]
            t=t_full[slice1][slice2]
            u=u_full[slice2, :][:, slice2][slice1, :][:, slice1]

        # Add other elif blocks here if needed for other datasets...

        elif dataset_name == 'NS':
             print("Navier-Stokes dataset requires separate handling - skipping subsampling.")
             return "Subsampling not applicable for NS.", None, None, None
        else:
            raise ValueError(f"Unknown dataset name: {dataset_name}")

        # --- Post-loading checks ---
        if u is None or x is None or t is None:
             raise ValueError(f"Data loading failed for dataset {dataset_name} - u, x, or t is None.")


        # import numpy as np
        # import json

        # --- Subsampling and Formatting ---
        print("Performing subsampling...")
        x = x.flatten()
        t = t.flatten()
        if u.shape[0] != len(x) or u.shape[1] != len(t):
            if u.T.shape == (len(x), len(t)):
                print("Warning: Transposing 'u' array.")
                u = u.T
            else:
                raise ValueError(f"Shape mismatch: u({u.shape}), x({len(x)}), t({len(t)})")

        n_x_full, n_t_full = u.shape
        if n_x_full < n_x_sub or n_t_full < n_t_sub:
            print(f"Warning: Full data smaller than subsample. Using full data.")
            n_x_sub, n_t_sub = n_x_full, n_t_full
        if n_x_sub < 1 or n_t_sub < 1:
            raise ValueError("Cannot subsample with zero dimensions.")

        x_indices = np.linspace(0, n_x_full - 1, n_x_sub, dtype=int)
        t_indices = np.linspace(0, n_t_full - 1, n_t_sub, dtype=int)
        subsampled_x = x[x_indices]
        subsampled_t = t[t_indices]
        subsampled_u = u[np.ix_(x_indices, t_indices)]

        dx_sub = float(np.mean(np.diff(subsampled_x))) if len(subsampled_x) > 1 else None
        dt_sub = float(np.mean(np.diff(subsampled_t))) if len(subsampled_t) > 1 else None

        # --- JSON Packaging ---
        subsample_json = {
            "dx": round(dx_sub, 4) if dx_sub is not None else None,
            "dt": round(dt_sub, 4) if dt_sub is not None else None,
            "x": [round(float(xi), 4) for xi in subsampled_x],
            "t": [round(float(ti), 4) for ti in subsampled_t],
            "u": [[round(float(subsampled_u[i, j]), 4) for j in range(len(subsampled_t))] for i in range(len(subsampled_x))]
        }

        # Example of storing to string or file:
        # json_string = json.dumps(subsample_json, indent=2)
        # with open("subsample_data.json", "w") as f:
        #     f.write(json_string)

        print("Formatted subsample JSON generated.")
        print(f"--- Finished processing: {dataset_name} ---")

        return subsample_json, u, x, t  # Return JSON dict and full data


        # # --- Subsampling and Formatting ---
        # print("Performing subsampling...")
        # x = x.flatten()
        # t = t.flatten()
        # if u.shape[0] != len(x) or u.shape[1] != len(t):
        #      if u.T.shape == (len(x), len(t)):
        #          print("Warning: Transposing 'u' array.")
        #          u = u.T
        #      else: raise ValueError(f"Shape mismatch: u({u.shape}), x({len(x)}), t({len(t)})")

        # n_x_full, n_t_full = u.shape
        # if n_x_full < n_x_sub or n_t_full < n_t_sub:
        #     print(f"Warning: Full data smaller than subsample. Using full data.")
        #     n_x_sub, n_t_sub = n_x_full, n_t_full
        # if n_x_sub < 1 or n_t_sub < 1:
        #     raise ValueError("Cannot subsample with zero dimensions.")


        # x_indices = np.linspace(0, n_x_full - 1, n_x_sub, dtype=int)
        # t_indices = np.linspace(0, n_t_full - 1, n_t_sub, dtype=int)
        # subsampled_x = x[x_indices]
        # subsampled_t = t[t_indices]
        # subsampled_u = u[np.ix_(x_indices, t_indices)]

        # lines = ["Raw Data Subsample (u[x, t]):", ""]
        # header = "| x \\ t | " + " | ".join([f"{ti:.3f}" for ti in subsampled_t]) + " |"
        # separator = "|---| " + " | ".join(["---"] * len(subsampled_t)) + " |"
        # lines.append(header); lines.append(separator)
        # for i, xi in enumerate(subsampled_x):
        #     row_data = [f"{xi:.2f}"] + [f"{subsampled_u[i, j]:.4f}" for j in range(len(subsampled_t))]
        #     lines.append("| " + " | ".join(row_data) + " |")

        # dx_sub = np.mean(np.diff(subsampled_x)) if len(subsampled_x) > 1 else 'N/A'
        # dt_sub = np.mean(np.diff(subsampled_t)) if len(subsampled_t) > 1 else 'N/A'
        # lines.append(f"\nApproximate dx in subsample: {dx_sub if isinstance(dx_sub, str) else f'{dx_sub:.4g}'}")
        # lines.append(f"Approximate dt in subsample: {dt_sub if isinstance(dt_sub, str) else f'{dt_sub:.4g}'}")
        # subsample_text = "\n".join(lines)
        # print("Formatted subsample text generated.")

        # print(f"--- Finished processing: {dataset_name} ---")
        # return subsample_text, u, x, t # Return subsample text and full data

    # --- Error Handling ---
    except FileNotFoundError as e: return f"Subsample Error: File not found ({e}).", None, None, None
    except KeyError as e: return f"Subsample Error: Variable key {e} not found.", None, None, None
    except ValueError as e: return f"Subsample Error: ValueError ({e}).", None, None, None
    except Exception as e: return f"Subsample Error: Unexpected error ({e}).", None, None, None


# ==============================================================
# LLM Prompting Function (Remains the same - Anonymous Subsample)
# ==============================================================
def get_candidate_eq_from_anonymous_subsample(operands, operators, subsample_text, llm):
    """
    Asks LLM to guess PDE based *directly* on an anonymous raw numerical subsample.
    Requires a pre-configured 'llm' object with a .predict method.
    """
    prompt = f"""
You are an AI assistant trying to identify a governing PDE from raw data.
You are given a small subsample of the raw numerical data `u(x, t)` from an unknown physical process, presented as a table:
<Important> Base your guesses on the provided data characteristics and common PDE forms like BURGERS, CHAFEE-INFANTE, KURAMOTOSIVASHINKY,
FISHER-KPP, NAVIER-STOKES, SCHRODINGER etc. The Kuramoto-Sivashinsky equation often involves terms like u*u_x, u_xx, and u_xxxx.
{subsample_text}

**Your Task:**
Analyze the numerical patterns **directly within this table**. Do NOT assume anything based on dataset origin - focus solely on the numbers provided.

1.  **Infer Changes:** Look at how `u` changes when `t` increases (for fixed `x` - hints at $u_t$) and how `u` changes when `x` increases
    (for fixed `t` - hints at $u_x$). Estimate the *local* rate of change from the numbers.
2.  **Infer Curvature/Higher Derivatives:** Look at how the change in `u` itself changes across `x` or `t`. Does the data look linear, curved,
    or oscillatory in space or time within this small window?
3.  **Infer Nonlinearity:** Does the change in `u` seem to depend on the value of `u` itself? (e.g., does `u` change faster where `u` is large?
    Hints at terms like $u*u_x$ or $u^2$).
4.  **Guess PDE:** Based *only* on the relationships you can infer from the raw numbers in the table, combined with general knowledge of common PDE structures
    (like diffusion, advection $u_x$, reaction $f(u)$, dispersion, nonlinear terms $u u_x, u^2$, etc.), propose 10 candidate equations of the form `u_t = ...`.
5.  **Estimate Coefficients:** Give the coeff which mostly fits with the data(run a least square on your own given this subsampled data)

**Symbol Library:** Use only symbols from `operands: {operands}` and `operators: {operators}`. Do permutation and combination within or between
the libraries given for forming mathematically and physically plausible PDE terms as well as PDEs.

**Output Format:** Provide ONLY the 10 equations, one per line. No explanations or code. Coefficients should be floating-point numbers.

Example Output:
u_t = [coeff] * [term] + [coeff] * [term] + ...
u_t = ...
"""

    print(f"--- Sending Anonymous Prompt to LLM via llm.predict ---")
    try:
        # !!! Assumes 'llm' is defined and configured correctly !!!
        response = llm.predict(prompt)
        print("--- Received Response from LLM ---")
    except NameError:
         print("ERROR: The 'llm' object is not defined. Cannot call the LLM.")
         response = ""
    except Exception as e:
        print(f"Error calling llm.predict: {e}")
        response = ""

    if response and isinstance(response, str):
         candidate_features = [line.strip() for line in response.splitlines() if line.strip()]
    else:
         print("LLM call failed or returned an empty/invalid response.")
         candidate_features = []
    return candidate_features




from together import Together

class ChatTogether:
    def __init__(self, model_name: str, api_key: Optional[str] = None, temperature: float = 0):
        self.model_name = model_name
        self.temperature = temperature
        self.client = Together(api_key=api_key)

    def predict(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
        )
        return response.choices[0].message.content.strip()
llm = ChatTogether(model_name="deepseek-ai/DeepSeek-V3", api_key="340c7af666664bd36e2cf94d10ee52a25a3439212668131235cd6a045a431a9a")
# response = llm.predict("What is the Schrdinger equation?")
# print(response)

def preprocess(llm_out):
        print("--- Preprocessing LLM Output ---")
        processed_list = []; eq_list_raw = []
        if isinstance(llm_out, str):
            if '<res>' in llm_out and '</res>' in llm_out:
                 print("  Extracting equations using <res> tags...");
                 try: eq_list_raw = extract_eqs(llm_out)
                 except NameError: print("  Warning: extract_eqs not found."); eq_list_raw = llm_out.split('\n')
            else: eq_list_raw = llm_out.split('\n')
        elif isinstance(llm_out, list): eq_list_raw = llm_out
        else: print("  Warning: Unexpected input type for preprocessing."); return []
        standard_eqs = []
        for i, eq in enumerate(eq_list_raw):
            expression = eq.strip();
            if not expression: continue
            expression_cleaned = expression;
            try:
                expression_cleaned = re.sub(r"^\s*\d+[\.:\)]?\s*", "", expression_cleaned)
                expression_cleaned = re.sub(r"^\s*[A-Za-z\s\-]+\s*[:\-]\s*", "", expression_cleaned)
                expression_cleaned = re.sub(r"^[Uu]_?t\s*=\s*", "", expression_cleaned).strip()
                expression_cleaned = expression_cleaned.replace('u*(1-u)', 'u-u^2')
                # *** ADDED CLEANING for trailing chars ***
                expression_cleaned = expression_cleaned.rstrip(',\'"')
                if expression_cleaned: print(f"  Cleaned Eq {i+1} RHS: {expression_cleaned}"); standard_eqs.append(expression_cleaned)
                else: print(f"  Skipping empty Eq {i+1} after cleaning: {expression}")
            except Exception as e: print(f"  Error preprocessing line: {eq}\n    Error: {e}")
        print(f"  Preprocessing resulted in {len(standard_eqs)} candidate RHS expressions.")
        return standard_eqs

def expression_with_coeff(expr, threshold=1e-2):
    """
    Filters out terms from a mathematical expression based on coefficient magnitude.

    Parameters:
        expr (str): A mathematical expression as a string.
        threshold (float): Minimum absolute value of coefficient to retain the term.

    Returns:
        str: Filtered expression string.
    """
    # Match patterns like: '(+0.1)*u_xx' or '(-3e-5)*u_xxxx'
    term_pattern = r'\([+-]?\d*\.?\d+(?:[eE][+-]?\d+)?\)\*[^+]+?(?=(\s\+\s\([+-])|$)'
    terms = re.findall(term_pattern, expr)

    filtered_terms = []
    for full_term in re.finditer(term_pattern, expr):
        term = full_term.group(0)
        # Extract the coefficient
        coeff_match = re.match(r'\(([+-]?\d*\.?\d+(?:[eE][+-]?\d+)?)\)', term)
        if coeff_match:
            coeff = float(coeff_match.group(1))
            if abs(coeff) > threshold:
                filtered_terms.append(term.strip())

    return ' + '.join(filtered_terms)

"""## Open-Router & TextGrad Setup

### Installations
"""

!pip install openai
!pip install textgrad
!pip install langchain
!pip install langchain_community
!pip install together

"""### Server-setup"""

import os
import re
import numpy as np
import scipy.io as sio
from langchain.chat_models import ChatOpenAI
import textgrad as tg
from openai import OpenAI
from textgrad.engine.local_model_openai_api import ChatExternalClient
from typing import Optional

"""### API-Key"""

# Define your API key here
API_KEY = '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'
os.environ['OPENROUTER_API_KEY'] = API_KEY
os.environ['openai_api_key'] = API_KEY

"""### Model-Selection

### Together AI
"""

from together import Together

class ChatTogether:
    def __init__(self, model_name: str, api_key: Optional[str] = None, temperature: float = 0):
        self.model_name = model_name
        self.temperature = temperature
        self.client = Together(api_key=api_key)

    def predict(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
        )
        return response.choices[0].message.content.strip()
llm = ChatTogether(model_name="deepseek-ai/DeepSeek-V3", api_key="1a93bcdf2722d7c7a97f19e2ead7ecfcfc4cd7e0b1ebdeddd515ffb8a6028502")
# response = llm.predict("What is the Schrdinger equation?")
# print(response)

# Extra_API_Keys1 = '97e95d65792aecb2160e4a0f6ce098e3a7e60c4474b33d4fdd07af9549db7ea2'
# Extra_API_Keys2 = '25332374e5bb427f49e9c9b3c124c2410f76b2ad1e3ddb7ae9dacf513c33221e'

"""# Main Function Call"""

# --- Add EDL directory to path ---
EDL_EVAL_PATH = '/content/EDL/evaluation' # Assumes EDL clone is in current dir

if not os.path.isdir(EDL_EVAL_PATH):
    print(f"ERROR: EDL evaluation directory not found at: {os.path.abspath(EDL_EVAL_PATH)}")
    sys.exit(1)
if EDL_EVAL_PATH not in sys.path: sys.path.append(EDL_EVAL_PATH)
print(f"Attempting to import from: {os.path.abspath(EDL_EVAL_PATH)}")

# --- Imports from EDL Evaluation (Selective) ---
try:
    # Import only necessary components for this workflow
    from sympy_utils import str2sympy, create_sympy_symbols, check_error, check_symbols_valid, count_functerms
    from sr_utils import make_metric, ScipyMinimize, merge_dict, reorganize, remove_redundants
    from expression import Equation, PriorityQueue
    # Bypassed: linear_calculate, TrainSTRidge, data_load, ODEData
    def extract_eqs(output,invalids=['{', "}"]): # Define locally
        eq_list = []; matches = re.findall("<res>(.*?)</res>" , output, re.DOTALL)
        eq_list = [match.strip() for match in matches]; eq_str = "\n".join(eq_list)
        for invalid in invalids: eq_str = eq_str.replace(invalid, "")
        return eq_str.split('\n')
    print("Successfully imported/defined EDL components.")
except ImportError as e: print(f"Error importing from EDL: {e}"); sys.exit(1)
except Exception as e: print(f"Unexpected error during EDL import: {e}"); sys.exit(1)

# '''
# # --- Finite Difference Helper Functions ---
# def FiniteDiff(u, dx, d):
#     n=u.size; ux=np.zeros(n,dtype=float)
#     if d==0: return u
#     min_points=max(9 if d>=4 else 7 if d>=3 else 5 if d>=2 else 3 if d>=1 else 1, 1)
#     if n < min_points: raise ValueError(f"Need >={min_points} points for d={d} FD, got {n}")
#     if d==1: ux[1:-1]=(u[2:]-u[:-2])/(2*dx); ux[0]=(-3./2*u[0]+2*u[1]-u[2]/2)/dx; ux[n-1]=(3./2*u[n-1]-2*u[n-2]+u[n-3]/2)/dx
#     elif d==2: ux[1:-1]=(u[2:]-2*u[1:-1]+u[:-2])/dx**2; ux[0]=(2*u[0]-5*u[1]+4*u[2]-u[3])/dx**2; ux[n-1]=(2*u[n-1]-5*u[n-2]+4*u[n-3]-u[n-4])/dx**2
#     elif d==3:
#         for i in range(2,n-2): ux[i]=(u[i+2]/2-u[i+1]+u[i-1]-u[i-2]/2)/dx**3
#         ux[0]=(-2.5*u[0]+9*u[1]-12*u[2]+7*u[3]-1.5*u[4])/dx**3; ux[1]=(-2.5*u[1]+9*u[2]-12*u[3]+7*u[4]-1.5*u[5])/dx**3
#         ux[n-1]=(2.5*u[n-1]-9*u[n-2]+12*u[n-3]-7*u[n-4]+1.5*u[n-5])/dx**3; ux[n-2]=(2.5*u[n-2]-9*u[n-3]+12*u[n-4]-7*u[n-5]+1.5*u[n-6])/dx**3
#     elif d==4:
#         try: uxx = FiniteDiff(u, dx, 2); uxxxx = FiniteDiff(uxx, dx, 2); ux = uxxxx
#         except ValueError as e: raise ValueError(f"Recursive FD failed for d=4: {e}")
#     elif d > 4: raise ValueError(f"FiniteDiff order d={d} not implemented beyond d=4.")
#     return ux

# def finite_diff_2d_wrapper(data, axis, diff_order, dx):
#     if data.ndim != 2: raise ValueError("Input data must be 2D")
#     output = np.zeros_like(data, dtype=float);
#     try:
#         if axis == 0: # Space axis
#             if diff_order == 0: return data.copy()
#             for j in range(data.shape[1]): output[:, j] = FiniteDiff(data[:, j], dx, diff_order)
#         elif axis == 1: # Time axis
#             if diff_order == 0: return data.copy()
#             for i in range(data.shape[0]): output[i, :] = FiniteDiff(data[i, :], dx, diff_order)
#         else: raise ValueError("Axis must be 0 or 1")
#         if np.isnan(output).any() or np.isinf(output).any():
#             print(f"    WARNING: NaN/Inf generated during FD (axis={axis}, order={diff_order}). Replacing with 0.")
#             np.nan_to_num(output, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
#     except ValueError as e: print(f"    ERROR in FiniteDiff (axis={axis}, order={diff_order}): {e}"); raise
#     return output

# # --- Term Evaluation (Direct Calculation) ---
# term_cache = {}
# def evaluate_term_direct(term_name: str, u_data: np.ndarray, x_grid_1d: np.ndarray, dx: float, dt: float) -> Optional[np.ndarray]:
#     """ Calculates the numerical value of a term string using u_data, x_grid. """
#     global term_cache
#     # Ensure term_name is base name (no leading sign)
#     term_name = term_name.strip().lstrip('+-')
#     if not term_name: return None # Skip if only sign was passed

#     if term_name in term_cache: return term_cache[term_name]
#     # print(f"      Calculating '{term_name}'...") # Keep commented unless deep debug needed
#     result = None; original_shape = u_data.shape
#     try:
#         if term_name == '1': result = np.ones_like(u_data, dtype=float)
#         elif term_name == 'u': result = u_data.copy().astype(float)
#         elif term_name == 'x':
#             if x_grid_1d.shape[0] != u_data.shape[0]: raise ValueError("x_grid shape mismatch")
#             result = np.tile(x_grid_1d.reshape(-1, 1), (1, u_data.shape[1])).astype(float)
#         elif '^' in term_name:
#             match = re.match(r"(.+)\^(\d+)", term_name)
#             if match:
#                 base, exponent_str = match.groups(); exponent = int(exponent_str)
#                 base_field = evaluate_term_direct(base, u_data, x_grid_1d, dx, dt)
#                 if base_field is None: return None
#                 result = np.power(base_field, exponent)
#             else: raise ValueError(f"Invalid power format '{term_name}'")
#         elif '*' in term_name:
#             factors = term_name.split('*');
#             result_accum = None
#             for i, factor in enumerate(factors):
#                 factor_field = evaluate_term_direct(factor, u_data, x_grid_1d, dx, dt)
#                 if factor_field is None: return None
#                 result_accum = factor_field.copy() if i == 0 else result_accum * factor_field
#             result = result_accum
#         elif '/' in term_name:
#             parts = term_name.split('/', 1);
#             if len(parts) == 2:
#                 num_name, den_name = parts;
#                 num_field = evaluate_term_direct(num_name, u_data, x_grid_1d, dx, dt)
#                 den_field = evaluate_term_direct(den_name, u_data, x_grid_1d, dx, dt)
#                 if num_field is None or den_field is None: return None
#                 den_abs_min = np.min(np.abs(den_field));
#                 if den_abs_min < 1e-9: print(f"        WARNING TermEval: Denominator near zero for '{term_name}'.")
#                 with warnings.catch_warnings():
#                     warnings.simplefilter("ignore", category=RuntimeWarning); result = num_field / (den_field + 1e-9 * np.sign(den_field + 1e-15))
#             else: raise ValueError(f"Invalid division format '{term_name}'")
#         elif term_name.startswith('u_'): # Derivatives
#             deriv_part = term_name[2:]; diff_order = len(deriv_part); axis = 0; step = dx
#             if diff_order > 0 and all(c == 'x' for c in deriv_part):
#                 base_u = evaluate_term_direct('u', u_data, x_grid_1d, dx, dt)
#                 if base_u is None: return None
#                 result = finite_diff_2d_wrapper(base_u, axis=axis, diff_order=diff_order, dx=step)
#             else: raise ValueError(f"Unknown derivative pattern '{term_name}'")
#         else: raise ValueError(f"Cannot parse/eval term '{term_name}'")

#         # --- Post-calculation Checks ---
#         if result is None: print(f"      DEBUG TermEval: Result is None for '{term_name}'")
#         elif result.shape != original_shape: print(f"      ERROR TermEval: Shape mismatch! '{term_name}' shape {result.shape}, expected {original_shape}"); return None
#         else:
#             nan_count = np.sum(np.isnan(result)); inf_count = np.sum(np.isinf(result))
#             if nan_count > 0 or inf_count > 0: print(f"      WARNING TermEval: Result for '{term_name}' contains {nan_count} NaN / {inf_count} Inf. Returning None."); return None
#             else: term_cache[term_name] = result
#         return result
#     except ValueError as ve: print(f"      Error evaluating term '{term_name}': {ve}"); return None
#     except Exception as e: print(f"      Unexpected error evaluating term '{term_name}': {e}"); traceback.print_exc(); return None

# # --- EDL Utility ---
# def replace_consts(input_string):
#     count = 0
#     def repl(match): nonlocal count; replacement = f"c_{count}"; count += 1; return replacement
#     result_string = re.sub(r'\bconst\b', repl, input_string)
#     return result_string, count

# # --- EDL Program Class ---
# class Program:
#     # Using definition provided by user
#     def __init__(self, expr, lhs, features):
#         self.optimizer = ScipyMinimize()
#         self.expr = expr; exp_str,count = replace_consts(expr)
#         try: self.eqs_sympy = sp.sympify(exp_str)
#         except (SyntaxError, TypeError, sp.SympifyError) as e: raise ValueError(f"Sympy parsing failed for '{exp_str}': {e}") from e
#         self.const_symbols = sp.symbols([f'c_{i}' for i in range(count)])
#         self.feature_names = list(features.keys()); self.var_symbols = sp.symbols(self.feature_names)
#         self.lhs = lhs.reshape(-1); self.y_rhs = [features[name] for name in self.feature_names]
#         self.init_const = [ random.uniform(-1, 1) for _ in range(count)]

#     def loss(self,rhs):
#         if rhs is None or not isinstance(rhs, np.ndarray) or np.any(np.isnan(rhs)) or np.any(np.isinf(rhs)): return 1e10
#         try: loss = np.mean(np.square(rhs-self.lhs))
#         except ValueError: return 1e10 # Shape mismatch
#         return loss if not (np.isnan(loss) or np.isinf(loss)) else 1e10

#     def process_sym(self, consts):
#         try:
#             const_subs = dict(zip(self.const_symbols, consts)); eq_subs = self.eqs_sympy.subs(const_subs)
#             f = sp.lambdify(self.var_symbols, eq_subs, ['numpy', {'conjugate': lambda x: x}])
#             rhs =f(*self.y_rhs); loss = self.loss(rhs)
#         except Exception: loss = 1e10
#         return loss

#     def rhs_evaluate(self,consts,y_rhs_features):
#         rhs = np.zeros_like(self.lhs)
#         try:
#             const_subs = dict(zip(self.const_symbols, consts)); eq_subs = self.eqs_sympy.subs(const_subs)
#             f = sp.lambdify(self.var_symbols, eq_subs, ['numpy', {'conjugate': lambda x: x}])
#             rhs =f(*y_rhs_features)
#             if np.any(np.isnan(rhs)) or np.any(np.isinf(rhs)): print(f"    WARNING: NaN/Inf in rhs_evaluate."); rhs[:] = 0
#         except Exception as e: print(f"    ERROR: rhs_evaluate Failed: {e}")
#         return rhs

#     def optimize_constants(self,):
#         consts=np.array(self.init_const)
#         # if len(self.init_const)>0:
#         #     try:
#         #          bounds = [(-10, 10)] * len(self.init_const)
#         #          if not callable(self.optimizer): raise TypeError("Optimizer not callable")
#         #          optim_result = self.optimizer(self.process_sym, self.init_const, bounds=bounds)
#         #          if isinstance(optim_result, np.ndarray): consts = optim_result
#         #          elif hasattr(optim_result, 'x'): consts = optim_result.x
#         #          else: raise TypeError(f"Unexpected optimizer result: {type(optim_result)}")
#         #          if np.any(np.isnan(consts)) or np.any(np.isinf(consts)): print("    WARNING: Optimizer returned NaN/Inf."); consts = np.array(self.init_const)
#         #     except Exception as e: print(f"    WARNING: Optimizer failed: {e}."); consts = np.array(self.init_const)
#         return consts

# # --- EDL Evaluator Class (Using Custom Linear Fit) ---
# class Evaluator:
#     cache = {}
#     def __init__(self, data_name, metric='inv_nrmse', metric_params=[], top_k=5, max_terms=10, l0_penalty=1e-5, mode='custom_lstsq', add_const=0, noise=0):
#         self.data_name = data_name; self.lhs = None; self.feature_dict = None; self.train_rhs = None; self.test_rhs = None; self.y_test = None; self.lhs_test = None; self.feature_names = []; self.features = []
#         self.max_terms = max_terms; self.l0_penalty = l0_penalty; self.mode = mode;
#         if not isinstance(metric_params, (list, tuple)): metric_params = [metric_params]
#         try: self.metrics = make_metric(metric, *metric_params); print(f"Evaluator initialized with metric: {metric}")
#         except NameError: print("CRITICAL ERROR: 'make_metric' not defined."); sys.exit(1)
#         except AssertionError as ae:
#              print(f"ERROR creating metric '{metric}': {ae}.")
#              fallback_metric = 'R2'; print(f"Defaulting to '{fallback_metric}' metric.")
#              try: self.metrics = make_metric(fallback_metric); self.metrics.__name__ = fallback_metric
#              except Exception as fallback_e:
#                   print(f"ERROR: Fallback metric '{fallback_metric}' failed: {fallback_e}");
#                   def mse_fallback(y_true, y_pred, complexity=0):
#                       if y_pred is None or not isinstance(y_pred, np.ndarray) or np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)): return float('inf')
#                       if y_true.shape != y_pred.shape: y_pred = np.resize(y_pred, y_true.shape)
#                       mse = np.mean(np.square(y_true - y_pred)); return mse if not (np.isnan(mse) or np.isinf(mse)) else float('inf')
#                   self.metrics = mse_fallback; self.metrics.__name__ = "MSE_Fallback"; print("Using basic MSE calculation.")
#         except Exception as e: print(f"ERROR creating metric '{metric}': {e}."); sys.exit(1)
#         self.invalid = {}; self.pq = PriorityQueue(top_k); self.add_const = add_const

#     def set_data(self, lhs_vector, feature_dictionary, train_lhs=None, test_lhs=None, test_features_dict=None):
#          self.lhs = lhs_vector; self.feature_dict = feature_dictionary; self.feature_names = list(self.feature_dict.keys()); self.features = [self.feature_dict[name] for name in self.feature_names]
#          self.train_rhs = train_lhs if train_lhs is not None else self.lhs; self.lhs_test = test_lhs
#          if test_features_dict: self.y_test = [test_features_dict[name] for name in self.feature_names]
#          else: self.y_test = None
#          self.test_rhs = test_lhs
#          print("Evaluator data set manually:"); print(f"  LHS shape: {self.lhs.shape if self.lhs is not None else 'None'}"); print(f"  Feature names: {self.feature_names}"); print(f"  Number of features: {len(self.features)}")

#     def evaluate_score_nonlinear(self,eq_list):
#         # Unchanged from previous version - uses Program class
#         equations = []; eq_scores = {}; invalid_reasons = {}
#         nonlinear_feature_names = list(self.feature_names) + ['const']; symbols = create_sympy_symbols(nonlinear_feature_names)
#         print(f"\n  Evaluating {len(eq_list)} NONLINEAR candidates...");
#         if self.lhs is None or self.feature_dict is None: print("  ERROR: Data not set."); return invalid_reasons, []
#         for i, eq_str_orig in enumerate(eq_list):
#             eq = eq_str_orig.strip(); score = float('inf'); optim_coefs=None; eq_sympy=None; len_ori=0; extra_metric={}; y_rhs_train=None
#             if not eq: continue; print(f"    Nonlinear candidate {i+1}: {eq}")
#             try:
#                 eq_sympy = str2sympy(eq, nonlinear_feature_names)
#                 string_error = check_symbols_valid(eq_sympy, symbols)
#                 if string_error : print(f"      Invalid symbol: {string_error}"); invalid_reasons[string_error] = 1+invalid_reasons.get(string_error,0); continue
#                 func_strs = count_functerms( eq_sympy); len_ori = len(func_strs)
#                 if len(func_strs)>10: print("      Skipping: Too many terms."); continue
#                 with warnings.catch_warnings():
#                     warnings.simplefilter("ignore"); p_eq = Program( eq, self.lhs, self.feature_dict)
#                     if len(p_eq.init_const)>6: print("      Skipping: Too many constants."); continue
#                     optim_coefs = p_eq.optimize_constants(); y_rhs_train = p_eq.rhs_evaluate(optim_coefs, self.features)
#                 score = self.metrics(self.lhs, y_rhs_train, len(func_strs))
#                 if np.isnan(score) or np.isinf(score): print("      Skipping: Score is NaN/Inf."); continue
#                 try: r2_train = r2_score(self.train_rhs, y_rhs_train); extra_metric['r2_train'] = r2_train
#                 except Exception as r2e: print(f"      Warning: R2 train failed: {r2e}"); extra_metric['r2_train'] = float('nan')
#                 if self.y_test is not None and self.test_rhs is not None:
#                     try: y_rhs_test = p_eq.rhs_evaluate(optim_coefs, self.y_test); r2_test = r2_score(self.test_rhs, y_rhs_test); extra_metric['r2_test'] = r2_test
#                     except Exception as r2etest: print(f"      Warning: R2 test failed: {r2etest}"); extra_metric['r2_test'] = float('nan')
#             except ValueError as e: print(f"      Skipping (ValueError): {e}"); invalid_reasons[f"ValueError: {e}"] = 1+invalid_reasons.get(f"ValueError: {e}",0); continue
#             except Exception as e2: print(f"      Skipping (Other Error): {e2}"); invalid_reasons[f"Exception: {e2}"] = 1+invalid_reasons.get(f"Exception: {e2}",0); continue
#             if eq not in eq_scores: eq_scores[eq] = round(score, 5); equation = Equation(eq, round(score,5), optim_coefs, eq_sympy, len_ori, extra_metric); equations.append(equation)
#         self.invalid = merge_dict(self.invalid, invalid_reasons); return invalid_reasons, equations

#     # --- evaluate_score_linear (REWRITTEN with fixes for parsing/evaluation) ---
#     def evaluate_score_linear(self, eq_list):
#         eq_scores = {}; invalid_reasons = {}; equations = []
#         symbols = create_sympy_symbols(self.feature_names) # Base symbols for parsing
#         # print(f"\n  Evaluating {len(eq_list)} LINEAR candidates (using CUSTOM direct lstsq)...");
#         if self.lhs is None or self.feature_dict is None: print("  ERROR: Data not set."); return invalid_reasons, []
#         expected_len = self.lhs.shape[0]
#         lhs_1d = self.lhs.reshape(-1)

#         for i, eq_str_orig in enumerate(eq_list):
#             eq = eq_str_orig.strip(); score=float('inf'); final_coef=None; eq_sympy_final=None; len_ori=0; extra_metrics={}; y_rhs=None; final_eq_str=eq; valid=True; base_term_names=[]
#             if not eq: continue; print(f"    Linear candidate {i+1}: {eq}")
#             global term_cache; term_cache.clear()

#             processed_func_terms = [] # Holds numerical data arrays
#             func_terms_names = [] # Holds string names of base terms

#             try:
#                 # --- Sympy Conversion ---
#                 eq_sympy_parsed = str2sympy(eq, self.feature_names) # Parse original string

#                 # *** Check Type AFTER str2sympy ***
#                 if not isinstance(eq_sympy_parsed, sp.Expr):
#                      print(f"      ERROR: str2sympy did not return valid sympy expr (got {type(eq_sympy_parsed)}). Skipping.")
#                      invalid_reasons[f"SympyType Error in {eq}"] = 1+invalid_reasons.get(f"SympyType Error in {eq}",0); continue

#                 # --- Term Extraction and Evaluation ---
#                 # Get terms, separating coefficients (like -1)
#                 if isinstance(eq_sympy_parsed, sp.Add): terms_sympy_with_coeffs = list(sp.Add.make_args(eq_sympy_parsed))
#                 elif isinstance(eq_sympy_parsed, (sp.Symbol, sp.Mul, sp.Pow, sp.Function, sp.Number, sp.Rational, sp.Float)): terms_sympy_with_coeffs = [eq_sympy_parsed]
#                 else: print(f"      Warning: Unexpected sympy type {type(eq_sympy_parsed)}. Skipping."); invalid_reasons[f"Unexpected Sympy Type in {eq}"] = 1+invalid_reasons.get(f"Unexpected Sympy Type in {eq}",0); continue

#                 valid_terms_check = True
#                 len_ori = len(terms_sympy_with_coeffs) # Original number of Add args

#                 term_dict_for_fit = {} # Store {base_term_name: term_data}

#                 for k, sub_expr in enumerate(terms_sympy_with_coeffs):
#                     # Separate numerical coefficient (like +1, -1) from the base term
#                     coeff_part, term_part = sub_expr.as_coeff_Mul() # Returns (coeff, rest)
#                     term_name_base = str(term_part).replace('**','^') # Get base term name string

#                     # print(f"      Term {k+1}: Expr='{sub_expr}', Coeff={coeff_part}, Base='{term_name_base}'") # Debug

#                     # Evaluate the BASE term using evaluate_term_direct
#                     term_data = evaluate_term_direct(term_name_base, u_data, x_flat, dx, dt)

#                     if term_data is None: print(f"      ERROR: evaluate_term_direct failed for base term '{term_name_base}'. Skipping PDE."); valid_terms_check = False; break
#                     term_data_flat = term_data.flatten()
#                     if term_data_flat.shape[0] != expected_len: print(f"      ERROR: Term '{term_name_base}' shape mismatch. Skipping."); valid_terms_check = False; break
#                     if np.isnan(term_data_flat).any() or np.isinf(term_data_flat).any(): print(f"      ERROR: Term '{term_name_base}' contains NaN/Inf. Skipping."); valid_terms_check = False; break

#                     # Store data associated with the base term name
#                     # If base term already seen (e.g., u_xx + 2*u_xx), lstsq will handle it
#                     if term_name_base not in term_dict_for_fit:
#                          term_dict_for_fit[term_name_base] = term_data_flat
#                          func_terms_names.append(term_name_base) # Keep track of unique base terms evaluated

#                 if not valid_terms_check: invalid_reasons[f"Term Eval/Shape/NaN Error in {eq}"] = 1+invalid_reasons.get(f"Term Eval/Shape/NaN Error in {eq}",0); continue

#                 # Get the data arrays in the correct order for the matrix
#                 processed_func_terms = [term_dict_for_fit[name] for name in func_terms_names]

#                 if not processed_func_terms: print("      Skipping: No valid terms evaluated."); continue

#                 # --- Remove Redundants (on evaluated data) ---
#                 # This check might be less necessary now, but keep for robustness
#                 processed_func_terms, func_terms_names, duplicates = remove_redundants(processed_func_terms, func_terms_names)
#                 if not processed_func_terms: print("      Skipping: No terms left after removing redundants."); continue

#                 # --- Direct Least Squares Fit ---
#                 # print(f"      Performing direct least squares with {len(processed_func_terms)} unique base terms...")
#                 rhs_matrix = np.array(processed_func_terms).T
#                 # print(f"      DEBUG: Assembled rhs matrix shape: {rhs_matrix.shape}") # Optional
#                 # print(f"      DEBUG: lhs vector shape: {lhs_1d.shape}") # Optional

#                 if rhs_matrix.shape[0] != lhs_1d.shape[0]: print(f"      ERROR: Shape mismatch RHS ({rhs_matrix.shape[0]}) vs LHS ({lhs_1d.shape[0]}). Skipping."); invalid_reasons[f"LHS/RHS Mismatch in {eq}"] = 1+invalid_reasons.get(f"LHS/RHS Mismatch in {eq}",0); continue

#                 try:
#                      coef, residuals, rank, s = np.linalg.lstsq(rhs_matrix, lhs_1d, rcond=None)
#                      valid = True; error_type = None
#                 except np.linalg.LinAlgError as lae: print(f"      Linear fit failed (LinAlgError): {lae}"); valid = False; error_type = f"LinAlgError: {lae}"; invalid_reasons[error_type] = 1+invalid_reasons.get(error_type,0); continue
#                 except ValueError as ve: print(f"      Linear fit failed (ValueError): {ve}"); valid = False; error_type = f"ValueError: {ve}"; invalid_reasons[error_type] = 1+invalid_reasons.get(error_type,0); continue

#                 # --- Pruning & Storing Results ---
#                 active_indices = [k for k, c in enumerate(coef) if abs(c) > 1e-7][:self.max_terms]
#                 if not active_indices: print("      Skipping: All coefficients zero after lstsq."); continue

#                 final_func_strs = [func_terms_names[k] for k in active_indices]; # Names of active terms
#                 final_coef = [coef[k] for k in active_indices] # Coeffs of active terms
#                 active_func_terms_data = [processed_func_terms[k] for k in active_indices] # Data of active terms

#                 if active_func_terms_data: y_rhs = np.sum([cf * tm for cf, tm in zip(final_coef, active_func_terms_data)], axis=0)
#                 else: y_rhs = np.zeros_like(lhs_1d)

#                 # Reconstruct final equation string and sympy expression from fitted terms/coeffs
#                 try:
#                     final_eq_str = " + ".join([f"({cf:+.7g})*{name}" for cf, name in zip(final_coef, final_func_strs)]).replace('+ -','- ')
#                     # Parse the *fitted* equation string to get a final sympy representation
#                     eq_sympy_final = str2sympy(final_eq_str, self.feature_names)
#                     if not isinstance(eq_sympy_final, sp.Expr): eq_sympy_final = eq_sympy_parsed # Fallback
#                 except Exception as reorg_e: print(f"      Warning: Reconstructing final eq string/sympy failed: {reorg_e}.") ; final_eq_str = eq; eq_sympy_final = eq_sympy_parsed # Fallback

#             # Catch errors from initial parsing or term evaluation
#             except SyntaxError as Se: print(f"      Skipping (Syntax Error): {Se}\n      Problem string: '{eq}'"); invalid_reasons[f"SyntaxError: {Se}"] = 1+invalid_reasons.get(f"SyntaxError: {Se}",0); continue
#             except (TypeError, ValueError, AttributeError, RecursionError) as parse_eval_e: print(f"      Skipping (Error during parsing/term eval): {parse_eval_e}"); invalid_reasons[f"{type(parse_eval_e).__name__}: {parse_eval_e}"] = 1+invalid_reasons.get(f"{type(parse_eval_e).__name__}: {parse_eval_e}",0); continue
#             except Exception as e: print(f"      Skipping (Unexpected Error): {e}"); invalid_reasons[f"Exception: {e}"] = 1+invalid_reasons.get(f"Exception: {e}",0); continue

#             # --- Scoring & Metrics ---
#             if valid and y_rhs is not None:
#                 try:
#                     score = self.metrics(lhs_1d, y_rhs, len(final_func_strs)) # Score based on N fitted terms
#                     if np.isnan(score) or np.isinf(score): print("      Skipping: Score is NaN/Inf."); continue
#                     try: r2_train = r2_score(self.train_rhs.reshape(-1), y_rhs); extra_metrics['r2_train'] = r2_train
#                     except Exception as r2e: print(f"      Warning: R2 train failed: {r2e}"); extra_metrics['r2_train'] = float('nan')
#                     # Test set evaluation would need modification if needed

#                     # Store results using final fitted equation string and coefficients
#                     if final_eq_str not in eq_scores: eq_scores[final_eq_str] = round(score,5); equation = Equation(final_eq_str, round(score,5), final_coef, eq_sympy_final, len_ori, extra_metrics); equations.append(equation)
#                 except Exception as score_e: print(f"      Skipping (Error during scoring/metrics): {score_e}"); invalid_reasons[f"Scoring Error: {score_e}"] = 1+invalid_reasons.get(f"Scoring Error: {score_e}",0)

#         self.invalid = merge_dict(self.invalid, invalid_reasons); return invalid_reasons, equations

#     # --- evaluate_score ---
#     def evaluate_score(self,llm_out):
#         eq_list = self.preprocess(llm_out); eq_linear,eq_nonlinear = [],[]
#         for eq in eq_list:
#             if len(eq)<1: continue
#             if 'const' in eq or re.search(r'\bc_\d+\b', eq): eq_nonlinear.append(eq)
#             else: eq_linear.append(eq)
#         _, eqs_linear = self.evaluate_score_linear(eq_linear) # Calls NEW linear evaluator
#         _, eqs_nonlinear = self.evaluate_score_nonlinear(eq_nonlinear) # Calls original nonlinear evaluator
#         eqs = eqs_linear + eqs_nonlinear; return len(eqs_nonlinear), eqs

#     # --- preprocess (Added cleaning for trailing chars) ---
#     def preprocess(self, llm_out):
#         print("--- Preprocessing LLM Output ---")
#         processed_list = []; eq_list_raw = []
#         if isinstance(llm_out, str):
#             if '<res>' in llm_out and '</res>' in llm_out:
#                  print("  Extracting equations using <res> tags...");
#                  try: eq_list_raw = extract_eqs(llm_out)
#                  except NameError: print("  Warning: extract_eqs not found."); eq_list_raw = llm_out.split('\n')
#             else: eq_list_raw = llm_out.split('\n')
#         elif isinstance(llm_out, list): eq_list_raw = llm_out
#         else: print("  Warning: Unexpected input type for preprocessing."); return []
#         standard_eqs = []
#         for i, eq in enumerate(eq_list_raw):
#             expression = eq.strip();
#             if not expression: continue
#             expression_cleaned = expression;
#             try:
#                 expression_cleaned = re.sub(r"^\s*\d+[\.:\)]?\s*", "", expression_cleaned)
#                 expression_cleaned = re.sub(r"^\s*[A-Za-z\s\-]+\s*[:\-]\s*", "", expression_cleaned)
#                 expression_cleaned = re.sub(r"^[Uu]_?t\s*=\s*", "", expression_cleaned).strip()
#                 expression_cleaned = expression_cleaned.replace('u*(1-u)', 'u-u^2')
#                 # *** ADDED CLEANING for trailing chars ***
#                 expression_cleaned = expression_cleaned.rstrip(',\'"')
#                 if expression_cleaned: print(f"  Cleaned Eq {i+1} RHS: {expression_cleaned}"); standard_eqs.append(expression_cleaned)
#                 else: print(f"  Skipping empty Eq {i+1} after cleaning: {expression}")
#             except Exception as e: print(f"  Error preprocessing line: {eq}\n    Error: {e}")
#         print(f"  Preprocessing resulted in {len(standard_eqs)} candidate RHS expressions.")
#         return standard_eqs

# '''

# === Main Execution ===
if __name__ == "__main__":
    # Initialize variables
    u_data_full, x_flat_full, t_flat_full = None, None, None; dx_full, dt_full = 1.0, 1.0
    u_data, x_flat, t_flat = None, None, None; dx, dt = 1.0, 1.0
    Ut, lhs_vector = None, None; feature_dict = None; eva = None; evaluated_eqs = None

    # u_data_full = np.load("/content/chafee_infante_CI.npy")
    # x_flat_full = np.load("/content/chafee_infante_x.npy").flatten
    # t_flat_full = np.load("/content/chafee_infante_t.npy").flatten()



    # 1. Load Data
    # DATA_FILE = '/content/fisher_nonlin_groundtruth.mat';
    # print(f"\n--- Loading Dataset: {DATA_FILE} ---")
    dataset = input(str("Enter Dataset Name : "))
    D_order_needed = 4
    try:

        feature_dict = {}
        if dataset == 'chafee-infante': # 301*200

            u_data_full = np.load("/content/chafee_infante_CI.npy")
            x_flat_full = np.load("/content/chafee_infante_x.npy").flatten()
            t_flat_full = np.load("/content/chafee_infante_t.npy").flatten()
            # n_input_var = 1
            # n, m = u.shape

        elif dataset == 'Burgers':

            # D_order_needed = 2

            data = sio.loadmat('/content/burgers.mat')
            u_data_full=data.get("usol")
            x_flat_full=data.get("x").flatten()
            t_flat_full=data.get("t").flatten()
            # sym_true = 'add,mul,u1,diff,u1,x1,diff2,u1,x1'
            # right_side_origin = 'right_side_origin = -1*u_origin*ux_origin+0.1*uxx_origin'
            # n_input_var = 1

        elif dataset == 'PDE_divide':
            u_data_full=np.load("/content/PDE_divide.npy").T[3:-3,3:-3]
            nx = 100
            nt = 251
            x_flat_full=np.linspace(1,2,nx).reshape(-1,1)[3:-3].flatten()
            t_flat_full=np.linspace(0,1,nt).reshape(-1,1)[3:-3].flatten()

            sym_true = 'add,div,diff,u1,x1,x1,diff2,u1,x1'
            right_side_origin = 'right_side_origin = -config.divide(ux_origin, x_all) + 0.25*uxx_origin'
            n_input_var = 1

        elif dataset == 'KS':
            data = sio.loadmat('/content/kuramoto_sivishinky.mat') # course temporal grid
            t_flat_full = data['tt'].flatten()
            x_flat_full = data['x'].flatten()
            u_data_full = data['uu']

            sym_true = 'add,mul,u1,diff,u1,x1,add,diff2,u1,x1,diff4,u1,x1'
            n_input_var = 1

        elif dataset == 'KS2':
            data = scipy.io.loadmat('./evaluation/data/KS.mat') # course temporal grid
            # import pdb;pdb.set_trace()

            t_flat_full = np.real(data['t'].flatten()[:,None])
            x_flat_full = np.real(data['x'].flatten()[:,None])
            u_data_full = np.real(data['usol'])

            sym_true = 'add,mul,u1,diff,u1,x1,add,diff2,u1,x1,diff2,diff2,u1,x1,x1'
            n_input_var = 1

        elif dataset == "fisher":

            data=sio.loadmat('/content/fisher_nonlin_groundtruth.mat')

            # D=data['D'] #0.02
            # r=data['r'] #10
            # K=data['K']
            # x_flat_full=np.squeeze(data['x'])[1:-1].reshape(-1,1)[3:-3].flatten()
            # t_flat_full=np.squeeze(data['t'])[1:-1].reshape(-1,1)[3:-3].flatten()
            # u_data_full=data['U'][3:-3,3:-3][1:-1,1:-1].T
            sym_true = "add,mul,u1,diff2,u1,x1,add,n2,diff,u1,x1,add,u1,n2,u1"
            n_input_var = 1

            x_flat_full=data['x'].flatten()
            t_flat_full=data['t'].flatten()
            u_data_full=data['U'].T



        # elif dataset == 'NS':
        #     w,u,v,w_t, w_x, w_xx, w_y, w_yy, xyt = load_ns_data()
        #     # feature_dict['x'] = x_data.reshape(-1)
        #     feature_dict['w_x'] = w_x.reshape(-1)
        #     feature_dict['w_xx'] = w_xx.reshape(-1)
        #     feature_dict['w_y'] = w_y.reshape(-1)
        #     feature_dict['w_yy'] = w_yy.reshape(-1)
        #     feature_dict['w'] = w.reshape(-1)
        #     feature_dict['u'] = u.reshape(-1)
        #     feature_dict['v'] = v.reshape(-1)
        #     return w_t.reshape(-1,1),  feature_dict

        elif dataset == 'kdv':

          data = sio.loadmat('/content/Kdv.mat')
          u_data_full = data['uu']
          x_flat_full = data['x'].flatten()
          t_flat_full = data['tt'].flatten()


        else:
            assert False, "Unknown dataset"

        # data = sio.loadmat(DATA_FILE);
        # u_data_full = np.load("/content/chafee_infante_CI.npy")
        # x_flat_full = np.load("/content/chafee_infante_x.npy").flatten()
        # t_flat_full = np.load("/content/chafee_infante_t.npy").flatten()

        # data = sio.loadmat('/content/Kdv.mat')
        # u_data_full = data['uu']
        # x_flat_full = data['x'][:,0]
        # t_flat_full = data['tt'][0,:]


        dx_full = np.mean(np.diff(x_flat_full)) if len(x_flat_full)>1 else 1.0; dt_full = np.mean(np.diff(t_flat_full)) if len(t_flat_full)>1 else 1.0
        print(f"Full data shapes: u={u_data_full.shape}, x={x_flat_full.shape}, t={t_flat_full.shape}"); print(f"Grid spacing: dx={dx_full:.4f}, dt={dt_full:.4f}")
        if u_data_full.ndim!=2 or u_data_full.shape[0]!=len(x_flat_full) or u_data_full.shape[1]!=len(t_flat_full): raise ValueError(f"Dim mismatch: u{u_data_full.shape}, x{len(x_flat_full)}, t{len(t_flat_full)}")
    except Exception as e: print(f"Data loading failed: {e}"); traceback.print_exc(); sys.exit(1)

    # 2. NO SUBSAMPLING
    print(f"\n--- Using Full Data (No Subsampling) ---")
    u_data = u_data_full; x_flat = x_flat_full; t_flat = t_flat_full; dx = dx_full; dt = dt_full
    print(f"Using data shape: u={u_data.shape}")
    min_spatial=max(9 if D_order_needed>=4 else 7 if D_order_needed>=3 else 5 if D_order_needed>=2 else 3 if D_order_needed>=1 else 1, 1); min_temporal=3
    if u_data.shape[0]<min_spatial or u_data.shape[1]<min_temporal: print(f"Error: Full data {u_data.shape} too small for D={D_order_needed}. Need ({min_spatial}, {min_temporal})."); sys.exit(1)

    # 3. Calculate LHS (Ut) on Full Data
    print("\n--- Calculating LHS (Ut) on Full Data ---")
    try:
        Ut = finite_diff_2d_wrapper(u_data, axis=1, diff_order=1, dx=dt); lhs_vector = Ut.flatten()
        print(f"Ut calculated, flat shape: {lhs_vector.shape}")
        if np.isnan(Ut).any() or np.isinf(Ut).any(): raise ValueError("NaN/Inf in Ut_gt.")
    except Exception as e: print(f"Fatal Error calculating Ut: {e}"); traceback.print_exc(); sys.exit(1)




    # 4. Calculate Base Features for feature_dict on Full Data
    print(f"\n--- Calculating Base Features (Up to D={D_order_needed}) on Full Data ---")
    feature_dict = {}
    try:
        for d in range(D_order_needed + 1):
            feature_name = 'u' if d == 0 else 'u_' + 'x' * d; print(f"  Calculating feature: {feature_name}")
            feature_data = finite_diff_2d_wrapper(u_data, axis=0, diff_order=d, dx=dx);
            feature_dict[feature_name] = feature_data.flatten() # Store flattened
            print(f"    Feature '{feature_name}' shape: {feature_dict[feature_name].shape}")
            nan_inf_check = np.isnan(feature_dict[feature_name]).any() or np.isinf(feature_dict[feature_name]).any()
            if nan_inf_check: print(f"    WARNING: NaN/Inf detected in feature '{feature_name}'!")
        # Add 'x' feature if needed
        # feature_dict['x'] = np.tile(x_flat.reshape(-1, 1), (1, u_data.shape[1])).flatten()
    except Exception as e: print(f"Fatal Error calculating features: {e}"); traceback.print_exc(); sys.exit(1)

    # 5. Define Candidate PDE String (Multi-line string from LLM)
    print("\n--- Using Provided PDE List for Evaluation ---")

    dataset_to_run = 'Burgers'
    operands = ['x', 'u', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx', 'u^2', 'u^3'] # Example library
    operators = ['+', '-', '*', '/']
    operands, operators = library(4, 4)
    subsample_prompt_text, u_full, x_full, t_full = load_pde_data_and_subsample(
        dataset_name=dataset_to_run,
        n_x_sub=100, # Adjust subsample size if needed
        n_t_sub=100
    )
    # print(subsample_prompt_text)
    pde_list = get_candidate_eq_from_anonymous_subsample(
            operands=operands,
            operators=operators,
            subsample_text=subsample_prompt_text,
            llm=llm # Pass the existing llm object
        )
    # Using the list that previously caused failures
    llm_output_string = pde_list # Added trailing quote to eq 1 to test syntax error fix
    print(llm_output_string)

    # 6. Instantiate and Configure EDL Evaluator
    print("\n--- Setting up EDL Evaluator ---")
    eva = None # Initialize
    try:
        chosen_metric = 'inv_nrmse' # Use valid metric name
        # Use custom mode name to signal using our direct lstsq implementation
        eva = Evaluator(data_name='burgers_custom_eval', metric=chosen_metric, mode='custom_lstsq', add_const=0)
        # print(f"INFO: Using Evaluator mode='{eva.mode}' (Custom Linear Fit)")
        eva.set_data(lhs_vector=lhs_vector, feature_dictionary=feature_dict)
    except NameError as ne: print(f"FATAL ERROR during Evaluator setup: Required function not found ({ne}). Check imports."); sys.exit(1)
    except Exception as e: print(f"Fatal Error setting up Evaluator: {e}"); traceback.print_exc(); sys.exit(1)

    # 7. Call Evaluation
    print("\n--- Running EDL Evaluation ---")
    evaluated_eqs = None # Initialize
    if eva is not None:
        try:
            # evaluate_score will call our modified evaluate_score_linear
            num_nonlinear, evaluated_eqs = eva.evaluate_score(llm_output_string)
            print(f"\nProcessed {len(evaluated_eqs) if evaluated_eqs is not None else 0} equations ({num_nonlinear} treated as nonlinear/const).")
        except Exception as e: print(f"Fatal Error during EDL evaluation: {e}"); traceback.print_exc(); sys.exit(1)
    else: print("Skipping evaluation because Evaluator setup failed.")

    # 8. Display Results
    print("\n\n--- EDL Evaluation Results ---")
    if evaluated_eqs is not None:
        evaluated_eqs.sort(key=lambda eq: eq.score if eq.score is not None and not np.isnan(eq.score) else float('inf'))
        print(f"\n--- Top {len(evaluated_eqs)} Evaluated Equations (Sorted by {eva.metrics.__name__}) ---")
        Scores = []
        pdeeq = []
        for i, eq_obj in enumerate(evaluated_eqs):
            print(f"\nRank {i+1}:")
            print(f"  Input Eq String: {eq_obj.exp_str}") # Corrected attribute
            pdeeq.append(eq_obj.exp_str)
            print(f"  Score ({eva.metrics.__name__}): {eq_obj.score:.6g}" if eq_obj.score is not None else "Score: None")
            Scores.append(round(eq_obj.score, 6))
            # Corrected attribute .coef
            if eq_obj.coef is not None and len(eq_obj.coef)>0:
                 coef_str = ", ".join([f"{c:.4f}" for c in eq_obj.coef]); #print(f"  Coefficients (from LSTSQ): [{coef_str}]") # Clarify source
            else: print("  Coefficients: None or Not Applicable")
            print(f"  Complexity (Original Terms): {eq_obj.len_ori}")
            # Corrected attribute .extra_metric and check if it's not None
            if hasattr(eq_obj, 'extra_metric') and eq_obj.extra_metric is not None:
                 metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in eq_obj.extra_metric.items() if v is not None]); print(f"  Extra Metrics: {{{metrics_str}}}")
            else: print("  Extra Metrics: None")
    else: print("No equations were successfully evaluated or evaluation did not run.")

    print("\n--- Invalid Equation Reasons Recorded ---")
    if eva is not None and hasattr(eva, 'invalid') and eva.invalid:
        sorted_invalid = sorted(eva.invalid.items(), key=lambda item: item[1], reverse=True)
        for reason, count in sorted_invalid:
            reason_str = str(reason).replace('\n', ' ');
            if len(reason_str) > 150: reason_str = reason_str[:147] + "..."
            print(f"  {reason_str}: {count} times")
    else: print("  No specific invalid reasons recorded (or evaluator failed).")

    max_epochs = 2
    # print(evaluated_eqs)

    for i in range(max_epochs):

      # b  = pri_eq_split(evaluated_eqs)
      # print(b)
      # Convert to dictionary (casting float64 to regular float for JSON compatibility)
      eq_score_dict = {eq: float(score) for eq, score in zip(pdeeq, Scores)}

      # Convert to JSON string (optional: indent for pretty formatting)
      json_str = json.dumps(eq_score_dict, indent=2)

      if (i%2) == 0 :

        si = self_improvement(json_str,operands,operators)
        print(si)
        llm_output_string = preprocess(si)
        print("--------------------------SELF-IMPROVEMENT---------------------------------------------------------")
        print(f"{len(llm_output_string)}")

      elif (i%2) == 1:

        ga = GA_by_llm(pdeeq,operands,operators)
        llm_output_string = preprocess(ga)
        print("--------------------------GA---------------------------------------------------------")
        print(f"{len(llm_output_string)}")


      # 5. Define Candidate PDE String (Multi-line string from LLM)
      # print("\n--- Using Provided PDE List for Evaluation ---")
      # pde_list = get_candidate_eq(operands, operators)
      # Using the list that previously caused failures
      # llm_output_string = pde_list # Added trailing quote to eq 1 to test syntax error fix

      # 6. Instantiate and Configure EDL Evaluator
      print(f"\n--- Setting up EDL Evaluator ---{i}")
      eva = None # Initialize
      try:
          chosen_metric = 'inv_nrmse' # Use valid metric name
          # Use custom mode name to signal using our direct lstsq implementation
          eva = Evaluator(data_name='burgers_custom_eval', metric=chosen_metric, mode='custom_lstsq', add_const=0)
          # print(f"INFO: Using Evaluator mode='{eva.mode}' (Custom Linear Fit)")
          eva.set_data(lhs_vector=lhs_vector, feature_dictionary=feature_dict)
      except NameError as ne: print(f"FATAL ERROR during Evaluator setup: Required function not found ({ne}). Check imports."); sys.exit(1)
      except Exception as e: print(f"Fatal Error setting up Evaluator: {e}"); traceback.print_exc(); sys.exit(1)

      # 7. Call Evaluation
      print(f"\n--- Running EDL Evaluation ---{i}")
      evaluated_eqs = None # Initialize
      if eva is not None:
          try:
              # evaluate_score will call our modified evaluate_score_linear
              num_nonlinear, evaluated_eqs = eva.evaluate_score(llm_output_string)
              # print(f"\nProcessed {len(evaluated_eqs) if evaluated_eqs is not None else 0} equations ({num_nonlinear} treated as nonlinear/const).")
          except Exception as e: print(f"Fatal Error during EDL evaluation: {e}"); traceback.print_exc(); sys.exit(1)
      else: print("Skipping evaluation because Evaluator setup failed.")

      # 8. Display Results
      print(f"\n\n--- EDL Evaluation Results ---{i}")
      if evaluated_eqs is not None:
          evaluated_eqs.sort(key=lambda eq: eq.score if eq.score is not None and not np.isnan(eq.score) else float('inf'))
          # print(f"\n--- Top {len(evaluated_eqs)} Evaluated Equations (Sorted by {eva.metrics.__name__}) ---")
          Scores = []
          pdeeq = []
          for i, eq_obj in enumerate(evaluated_eqs):
              # print(f"\nRank {i+1}:")
              # print(f"  Input Eq String: {eq_obj.exp_str}") # Corrected attribute
              pdeeq.append(eq_obj.exp_str)
              # print(f"  Score ({eva.metrics.__name__}): {eq_obj.score:.6g}" if eq_obj.score is not None else "Score: None")
              Scores.append(round(eq_obj.score, 6))
              # Corrected attribute .coef
              if eq_obj.coef is not None and len(eq_obj.coef)>0:
                  coef_str = ", ".join([f"{c:.4f}" for c in eq_obj.coef]); #print(f"  Coefficients (from LSTSQ): [{coef_str}]") # Clarify source
              else: print("  Coefficients: None or Not Applicable")
              # print(f"  Complexity (Original Terms): {eq_obj.len_ori}")
              # Corrected attribute .extra_metric and check if it's not None
              if hasattr(eq_obj, 'extra_metric') and eq_obj.extra_metric is not None:
                  metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in eq_obj.extra_metric.items() if v is not None]);# print(f"  Extra Metrics: {{{metrics_str}}}")
              else: print("  Extra Metrics: None")
      else: print("No equations were successfully evaluated or evaluation did not run.")

      print("\n--- Invalid Equation Reasons Recorded ---")
      if eva is not None and hasattr(eva, 'invalid') and eva.invalid:
          sorted_invalid = sorted(eva.invalid.items(), key=lambda item: item[1], reverse=True)
          for reason, count in sorted_invalid:
              reason_str = str(reason).replace('\n', ' ');
              if len(reason_str) > 150: reason_str = reason_str[:147] + "..."
              print(f"  {reason_str}: {count} times")
      else: print("  No specific invalid reasons recorded (or evaluator failed).")

pdeeq

Scores

print(expression_with_coeff(pdeeq[-1]))

"""## Plotting"""

import matplotlib.pyplot as plt
import numpy as np

# PDE names
pdes = ['Burgers', 'Chafee-Infante', 'KS', 'PDE_divide', 'Fisher-KPP', 'KdV']

# Coefficient error means and standard deviations
research_means = [1.25, 0.05, 0.5, 0.15, 1.34, np.nan]  # NaN for KdV
research_stds = [1.63, 0.03, 0.2, 0.09, 0.38, np.nan]

ours_means = [1.0, 1.5, 1.6, 3.6, 1.7, 2.7]
ours_stds = [1.0, 0.6, 0.9, 1.1, 0.5, 1.3]

# X axis
x = np.arange(len(pdes))

# Convert to numpy arrays for masking NaNs
research_means = np.array(research_means)
research_stds = np.array(research_stds)

# Plot
plt.figure(figsize=(10, 6))

# Research paper line and shaded region
valid_research = ~np.isnan(research_means)
plt.plot(x[valid_research], research_means[valid_research], '-o', label='Research Paper', color='blue')
plt.fill_between(
    x[valid_research],
    research_means[valid_research] - research_stds[valid_research],
    research_means[valid_research] + research_stds[valid_research],
    color='blue',
    alpha=0.2
)

# Ours line and shaded region
plt.plot(x, ours_means, '-s', label='Ours', color='orange')
plt.fill_between(
    x,
    np.array(ours_means) - np.array(ours_stds),
    np.array(ours_means) + np.array(ours_stds),
    color='orange',
    alpha=0.2
)

# Labels and title
plt.xticks(x, pdes, rotation=45)
plt.ylabel('Coefficient Error (%)')
plt.title('Comparison of Coefficient Errors')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
